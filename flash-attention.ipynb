{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e090b9-a9ca-4510-997e-425fc96d2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"1\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.language.extra import libdevice\n",
    "from flash_attn.flash_attn_interface import flash_attn_qkvpacked_func as flash_attn_func\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:6\")\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdd7bfc-2724-43a4-b4d3-93cdb4605825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_scaled_dot_product_attention(Q, K, V, is_causal=False):\n",
    "    if is_causal:\n",
    "        mask = torch.ones(Q.size(1), K.size(1)) * float(\"-inf\")\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "    attn = Q @ K.transpose(-2, -1)\n",
    "    attn = attn.to(torch.float32)\n",
    "    attn = attn / math.sqrt(Q.size(-1))\n",
    "    if is_causal:\n",
    "        attn += mask\n",
    "    attn = torch.softmax(attn, dim=-1)\n",
    "    attn = attn.to(torch.float16)\n",
    "    attn = attn @ V\n",
    "    return attn\n",
    "\n",
    "\n",
    "def my_scaled_sigmoid_dot_product_attention(Q, K, V, is_causal=False):\n",
    "    if is_causal:\n",
    "        mask = torch.ones(Q.size(1), K.size(1)) * float(\"-inf\")\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "    attn = Q @ K.transpose(-2, -1)\n",
    "    attn = attn.to(torch.float32)\n",
    "    attn = attn / math.sqrt(Q.size(-1))\n",
    "    if is_causal:\n",
    "        attn += mask\n",
    "    attn = torch.sigmoid(attn)\n",
    "    attn = attn.to(torch.float16)\n",
    "    attn = attn @ V\n",
    "    return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46f229e-9602-41d7-ab24-acdbdfa8f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    triton.Config({'Q_BLOCK_SIZE': BM, 'KV_BLOCK_SIZE': BN}, num_stages=s, num_warps=w) \n",
    "    for BM in [64, 128]\n",
    "    for BN in [32, 64]\n",
    "    for s in [3, 4, 7]\n",
    "    for w in [4, 8]\n",
    "]\n",
    "\n",
    "@triton.autotune(configs, key=[\"seq_len\", \"embed_dim\"])\n",
    "@triton.jit\n",
    "def flash_attention(q_ptr, k_ptr, v_ptr, o_ptr, seq_len, attn_norm, embed_dim, embed_dim_block: tl.constexpr, EMBED_DIM_IS_EVEN: tl.constexpr, Q_BLOCK_SIZE: tl.constexpr, KV_BLOCK_SIZE: tl.constexpr):\n",
    "    pid_0 = tl.program_id(0)\n",
    "    pid_1 = tl.program_id(1)\n",
    "\n",
    "    if EMBED_DIM_IS_EVEN:\n",
    "        embed_dim = embed_dim_block  # hint to compiler they are the same value to simplify mask calculation\n",
    "\n",
    "    q_block_ptr = tl.make_block_ptr(\n",
    "        base=q_ptr + pid_1 * seq_len * embed_dim,\n",
    "        shape=(seq_len, embed_dim),\n",
    "        strides=(embed_dim, 1),\n",
    "        offsets=(pid_0 * Q_BLOCK_SIZE, 0),\n",
    "        block_shape=(Q_BLOCK_SIZE, embed_dim_block),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    # loading a block pointer with reverted order results in less memory movement and better runtime as compared to transposing later on\n",
    "    k_block_ptr = tl.make_block_ptr(\n",
    "        base=k_ptr + pid_1 * seq_len * embed_dim,\n",
    "        shape=(embed_dim, seq_len),\n",
    "        strides=(1, embed_dim),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(embed_dim_block, KV_BLOCK_SIZE),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    v_block_ptr = tl.make_block_ptr(\n",
    "        base=v_ptr + pid_1 * seq_len * embed_dim,\n",
    "        shape=(seq_len, embed_dim),\n",
    "        strides=(embed_dim, 1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(KV_BLOCK_SIZE, embed_dim_block),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    o_block_ptr = tl.make_block_ptr(\n",
    "        base=o_ptr + pid_1 * seq_len * embed_dim,\n",
    "        shape=(seq_len, embed_dim),\n",
    "        strides=(embed_dim, 1),\n",
    "        offsets=(pid_0 * Q_BLOCK_SIZE, 0),\n",
    "        block_shape=(Q_BLOCK_SIZE, embed_dim_block),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    q = tl.load(q_block_ptr)\n",
    "    attn_norm = attn_norm.to(tl.float16)  \n",
    "    q = q * attn_norm  # precompute softmax scaling on Q so we don't have to do it inside the loop\n",
    "    m = tl.full((Q_BLOCK_SIZE,), -float(\"inf\"), dtype=tl.float32)\n",
    "    l = tl.zeros((Q_BLOCK_SIZE,), dtype=tl.float32)\n",
    "    o = tl.zeros((Q_BLOCK_SIZE, embed_dim_block), dtype=tl.float32)\n",
    "        \n",
    "    for i in range(0, seq_len, KV_BLOCK_SIZE):\n",
    "        seq_len = tl.multiple_of(seq_len, KV_BLOCK_SIZE)\n",
    "        k = tl.load(k_block_ptr)\n",
    "        v = tl.load(v_block_ptr)\n",
    "        \n",
    "        s = tl.dot(q, k)\n",
    "        old_m = m\n",
    "        m = tl.maximum(m, tl.max(s, -1))\n",
    "        s = s - m[:, None]\n",
    "        p = tl.exp2(s)  # Use exp2 to match GPU implementation. Log2e gets folded into softmax scale to save computation\n",
    "        m_shift = tl.exp2(old_m - m)\n",
    "        l = l * m_shift + tl.sum(p, -1)   \n",
    "        p = p.to(tl.float16)\n",
    "        o = o * m_shift[:, None]\n",
    "        o += tl.dot(p, v)\n",
    "\n",
    "        k_block_ptr = tl.advance(k_block_ptr, (0, KV_BLOCK_SIZE))\n",
    "        v_block_ptr = tl.advance(v_block_ptr, (KV_BLOCK_SIZE, 0))\n",
    "        \n",
    "    o = o / l[:, None]\n",
    "    o = o.to(tl.float16)\n",
    "\n",
    "    tl.store(o_block_ptr, o)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30caf916-54f3-474e-9696-b4c23634902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_flash_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, attn_norm):\n",
    "    batch_size = q.size(0)\n",
    "    num_heads = q.size(1)\n",
    "    seq_len = q.size(2)\n",
    "    embed_dim = q.size(3)\n",
    "    batch_size_gemm = batch_size * num_heads\n",
    "    embed_dim_block = triton.next_power_of_2(embed_dim)\n",
    "    if embed_dim == embed_dim_block:\n",
    "        EMBED_DIM_EVEN = True\n",
    "    else:\n",
    "        EMBED_DIM_EVEN = False\n",
    "    o = torch.empty_like(q)\n",
    "    # launching programs in batch minor order results in more KV data going through L2 cache and significantly better L2 cache hit rates\n",
    "    grid = lambda args: (triton.cdiv(seq_len, args[\"Q_BLOCK_SIZE\"]), batch_size_gemm)\n",
    "    kernel = flash_attention[grid](q, k, v, o, seq_len, attn_norm, embed_dim, embed_dim_block, EMBED_DIM_EVEN) \n",
    "    #print(kernel.asm['ttir'])\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4833f44c-faa7-450d-ae3e-05ee012db65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Triton autotuning for function flash_attention finished after 4.25s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None;\n",
      "torch.Size([4, 32, 4096, 64])\n",
      "torch.Size([4, 32, 4096, 64])\n",
      "torch.float16\n",
      "torch.float16\n",
      "The maximum difference between torch and triton is 0.00054931640625\n",
      "The mean difference between torch and triton is 1.33514404296875e-05\n",
      "tensor([[[[False, False,  True,  ..., False, False, False],\n",
      "          [False, False,  True,  ..., False, False, False],\n",
      "          [False, False,  True,  ...,  True, False, False],\n",
      "          ...,\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [False,  True,  True,  ..., False, False,  True]],\n",
      "\n",
      "         [[ True, False, False,  ...,  True, False, False],\n",
      "          [False,  True,  True,  ..., False,  True,  True],\n",
      "          [False, False, False,  ..., False, False,  True],\n",
      "          ...,\n",
      "          [ True, False, False,  ..., False,  True,  True],\n",
      "          [False, False,  True,  ..., False, False,  True],\n",
      "          [False, False, False,  ...,  True,  True, False]],\n",
      "\n",
      "         [[False,  True, False,  ..., False, False, False],\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [False,  True,  True,  ..., False, False, False],\n",
      "          ...,\n",
      "          [ True,  True, False,  ..., False, False, False],\n",
      "          [ True,  True, False,  ...,  True, False, False],\n",
      "          [ True, False, False,  ..., False,  True, False]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ True, False,  True,  ...,  True, False, False],\n",
      "          [ True,  True, False,  ...,  True, False, False],\n",
      "          [False,  True, False,  ...,  True,  True,  True],\n",
      "          ...,\n",
      "          [False, False,  True,  ...,  True, False, False],\n",
      "          [False, False, False,  ..., False,  True, False],\n",
      "          [ True, False,  True,  ..., False,  True, False]],\n",
      "\n",
      "         [[False, False, False,  ..., False, False, False],\n",
      "          [False,  True, False,  ...,  True, False, False],\n",
      "          [False, False, False,  ...,  True, False, False],\n",
      "          ...,\n",
      "          [False, False, False,  ...,  True,  True, False],\n",
      "          [ True, False, False,  ..., False, False, False],\n",
      "          [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "         [[False,  True, False,  ..., False, False, False],\n",
      "          [False,  True, False,  ..., False,  True, False],\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          ...,\n",
      "          [ True,  True, False,  ..., False, False,  True],\n",
      "          [False, False, False,  ..., False, False,  True],\n",
      "          [ True, False, False,  ..., False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True, False,  True,  ...,  True, False, False],\n",
      "          [False,  True, False,  ..., False, False, False],\n",
      "          [ True, False, False,  ..., False, False, False],\n",
      "          ...,\n",
      "          [False, False, False,  ...,  True, False, False],\n",
      "          [ True,  True, False,  ...,  True,  True, False],\n",
      "          [ True, False,  True,  ..., False, False, False]],\n",
      "\n",
      "         [[False, False, False,  ...,  True, False,  True],\n",
      "          [False,  True, False,  ...,  True,  True, False],\n",
      "          [False,  True,  True,  ..., False,  True,  True],\n",
      "          ...,\n",
      "          [False, False,  True,  ..., False, False, False],\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [False, False,  True,  ..., False,  True, False]],\n",
      "\n",
      "         [[ True, False,  True,  ..., False, False, False],\n",
      "          [ True, False,  True,  ...,  True, False, False],\n",
      "          [False,  True,  True,  ..., False, False,  True],\n",
      "          ...,\n",
      "          [ True, False, False,  ..., False,  True, False],\n",
      "          [False,  True,  True,  ..., False,  True,  True],\n",
      "          [False,  True, False,  ...,  True,  True,  True]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ True, False, False,  ..., False, False, False],\n",
      "          [ True,  True,  True,  ...,  True, False, False],\n",
      "          [False, False, False,  ..., False, False,  True],\n",
      "          ...,\n",
      "          [False, False, False,  ...,  True,  True,  True],\n",
      "          [ True, False,  True,  ...,  True, False, False],\n",
      "          [False, False, False,  ...,  True, False,  True]],\n",
      "\n",
      "         [[ True, False, False,  ..., False,  True, False],\n",
      "          [ True, False,  True,  ..., False, False, False],\n",
      "          [ True, False, False,  ..., False, False, False],\n",
      "          ...,\n",
      "          [False,  True, False,  ...,  True, False, False],\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [False,  True, False,  ..., False, False, False]],\n",
      "\n",
      "         [[False, False, False,  ...,  True, False, False],\n",
      "          [ True, False, False,  ..., False, False, False],\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          ...,\n",
      "          [False,  True,  True,  ...,  True, False,  True],\n",
      "          [ True, False, False,  ...,  True,  True, False],\n",
      "          [False, False, False,  ...,  True,  True, False]]],\n",
      "\n",
      "\n",
      "        [[[False, False,  True,  ..., False, False,  True],\n",
      "          [ True, False, False,  ..., False, False, False],\n",
      "          [False, False, False,  ...,  True, False, False],\n",
      "          ...,\n",
      "          [ True, False, False,  ..., False, False, False],\n",
      "          [False, False, False,  ..., False, False,  True],\n",
      "          [ True, False, False,  ..., False, False,  True]],\n",
      "\n",
      "         [[False, False,  True,  ..., False, False,  True],\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [ True, False,  True,  ..., False, False,  True],\n",
      "          ...,\n",
      "          [False, False, False,  ...,  True, False, False],\n",
      "          [False, False,  True,  ..., False, False, False],\n",
      "          [False, False, False,  ...,  True,  True, False]],\n",
      "\n",
      "         [[False, False, False,  ...,  True, False, False],\n",
      "          [False, False,  True,  ...,  True, False, False],\n",
      "          [False,  True, False,  ...,  True, False, False],\n",
      "          ...,\n",
      "          [ True,  True,  True,  ..., False, False,  True],\n",
      "          [False,  True, False,  ..., False,  True, False],\n",
      "          [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[False,  True, False,  ...,  True, False, False],\n",
      "          [False, False, False,  ...,  True,  True, False],\n",
      "          [False, False,  True,  ..., False, False, False],\n",
      "          ...,\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [False, False, False,  ...,  True, False, False],\n",
      "          [False,  True, False,  ..., False, False, False]],\n",
      "\n",
      "         [[ True,  True, False,  ..., False, False, False],\n",
      "          [ True, False,  True,  ..., False, False, False],\n",
      "          [ True, False, False,  ...,  True, False,  True],\n",
      "          ...,\n",
      "          [False, False,  True,  ..., False,  True, False],\n",
      "          [ True, False,  True,  ..., False,  True, False],\n",
      "          [False,  True,  True,  ..., False,  True, False]],\n",
      "\n",
      "         [[False, False, False,  ..., False, False,  True],\n",
      "          [False, False, False,  ..., False,  True, False],\n",
      "          [ True,  True, False,  ...,  True,  True, False],\n",
      "          ...,\n",
      "          [ True, False, False,  ..., False,  True, False],\n",
      "          [False, False, False,  ..., False,  True, False],\n",
      "          [False, False, False,  ..., False, False,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False,  True,  ..., False, False, False],\n",
      "          [False,  True, False,  ..., False, False, False],\n",
      "          [False,  True, False,  ..., False,  True,  True],\n",
      "          ...,\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [ True, False, False,  ..., False, False, False],\n",
      "          [False, False,  True,  ..., False, False,  True]],\n",
      "\n",
      "         [[False, False,  True,  ..., False, False,  True],\n",
      "          [ True,  True,  True,  ...,  True, False,  True],\n",
      "          [False, False,  True,  ...,  True, False, False],\n",
      "          ...,\n",
      "          [False,  True, False,  ..., False, False, False],\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [ True, False,  True,  ..., False, False,  True]],\n",
      "\n",
      "         [[False,  True, False,  ..., False, False, False],\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [ True, False, False,  ..., False, False, False],\n",
      "          ...,\n",
      "          [False,  True, False,  ..., False, False, False],\n",
      "          [False,  True, False,  ...,  True,  True, False],\n",
      "          [False,  True, False,  ...,  True, False,  True]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[False, False, False,  ..., False, False,  True],\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [ True, False,  True,  ..., False, False, False],\n",
      "          ...,\n",
      "          [False, False,  True,  ..., False,  True, False],\n",
      "          [False, False,  True,  ...,  True,  True, False],\n",
      "          [False, False,  True,  ..., False, False, False]],\n",
      "\n",
      "         [[ True, False,  True,  ..., False, False, False],\n",
      "          [ True, False, False,  ..., False,  True,  True],\n",
      "          [False, False,  True,  ...,  True, False, False],\n",
      "          ...,\n",
      "          [False, False,  True,  ..., False, False, False],\n",
      "          [False,  True,  True,  ..., False, False,  True],\n",
      "          [False,  True, False,  ...,  True, False, False]],\n",
      "\n",
      "         [[False, False,  True,  ..., False, False, False],\n",
      "          [False, False,  True,  ..., False, False, False],\n",
      "          [ True,  True,  True,  ..., False, False, False],\n",
      "          ...,\n",
      "          [False, False, False,  ..., False, False, False],\n",
      "          [False,  True,  True,  ...,  True, False, False],\n",
      "          [False,  True, False,  ..., False,  True, False]]]], device='cuda:6')\n",
      "tensor([[ 0.0057, -0.0013, -0.0252,  ..., -0.0265, -0.0082,  0.0250],\n",
      "        [-0.0100,  0.0234, -0.0184,  ..., -0.0118, -0.0201, -0.0054],\n",
      "        [-0.0133, -0.0026, -0.0311,  ...,  0.0282,  0.0126,  0.0145],\n",
      "        ...,\n",
      "        [ 0.0059,  0.0044, -0.0417,  ...,  0.0094,  0.0242,  0.0084],\n",
      "        [-0.0318, -0.0062, -0.0077,  ..., -0.0008,  0.0627,  0.0333],\n",
      "        [ 0.0176,  0.0328, -0.0252,  ...,  0.0074,  0.0018,  0.0247]],\n",
      "       device='cuda:6', dtype=torch.float16)\n",
      "tensor([[ 0.0057, -0.0012, -0.0252,  ..., -0.0265, -0.0082,  0.0249],\n",
      "        [-0.0099,  0.0234, -0.0184,  ..., -0.0118, -0.0200, -0.0054],\n",
      "        [-0.0133, -0.0025, -0.0311,  ...,  0.0282,  0.0127,  0.0145],\n",
      "        ...,\n",
      "        [ 0.0059,  0.0044, -0.0417,  ...,  0.0094,  0.0241,  0.0085],\n",
      "        [-0.0318, -0.0063, -0.0078,  ..., -0.0007,  0.0626,  0.0333],\n",
      "        [ 0.0176,  0.0328, -0.0252,  ...,  0.0074,  0.0018,  0.0247]],\n",
      "       device='cuda:6', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_heads = 32\n",
    "seq_len = 4096\n",
    "embed_dim = 64\n",
    "log2_e = 1.44269504\n",
    "attn_norm = log2_e / math.sqrt(embed_dim)\n",
    "#attn_norm = torch.tensor(attn_norm, dtype=torch.float16, device=device)\n",
    "\n",
    "q = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "k = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "v = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "\n",
    "output_torch = my_scaled_dot_product_attention(q, k, v) #.transpose(0, 1)\n",
    "output_triton = apply_flash_attention(q, k, v, attn_norm) #.transpose(0, 1)\n",
    "print(output_torch.size())\n",
    "print(output_triton.size())\n",
    "print(output_torch.dtype)\n",
    "print(output_triton.dtype)\n",
    "\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(output_torch - output_triton))}')\n",
    "print(f'The mean difference between torch and triton is '\n",
    "      f'{torch.mean(torch.abs(output_torch - output_triton))}')\n",
    "print(output_torch == output_triton)\n",
    "#print(q[0, 0, 0])\n",
    "print(output_torch[0, 0])\n",
    "print(output_triton[0, 0])\n",
    "#print(output_torch[0, 2] == output_triton[0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be7e43-1138-405b-8720-e72adf5ede0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d8d30-a1e8-424e-a4f8-eb70e973b841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68d5a08-656d-473c-b67f-f4ee9ed03c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    triton.Config({'Q_BLOCK_SIZE': BM, 'KV_BLOCK_SIZE': BN}, num_stages=s, num_warps=w) \n",
    "    for BM in [64, 128]\n",
    "    for BN in [32, 64]\n",
    "    for s in [3, 4, 7]\n",
    "    for w in [4, 8]\n",
    "]\n",
    "\n",
    "@triton.autotune(configs, key=[\"seq_len\", \"embed_dim\"])\n",
    "@triton.jit\n",
    "def flash_sigmoid_attention(q_ptr, k_ptr, v_ptr, o_ptr, batch_size, seq_len, attn_norm, embed_dim, embed_dim_block: tl.constexpr, EMBED_DIM_IS_EVEN: tl.constexpr, Q_BLOCK_SIZE: tl.constexpr, KV_BLOCK_SIZE: tl.constexpr):\n",
    "    pid_0 = tl.program_id(0)\n",
    "    pid_1 = tl.program_id(1)\n",
    "\n",
    "    if EMBED_DIM_IS_EVEN:\n",
    "        embed_dim = embed_dim_block  # set embed_dim and embed_dim_block to be the same constant to simplify mask calculations\n",
    "\n",
    "    \n",
    "    q_block_ptr = tl.make_block_ptr(\n",
    "        base=q_ptr + pid_1 * seq_len * embed_dim,\n",
    "        shape=(seq_len, embed_dim),\n",
    "        strides=(embed_dim, 1),\n",
    "        offsets=(pid_0 * Q_BLOCK_SIZE, 0),\n",
    "        block_shape=(Q_BLOCK_SIZE, embed_dim_block),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    k_block_ptr = tl.make_block_ptr(\n",
    "        base=k_ptr + pid_1 * seq_len * embed_dim,\n",
    "        shape=(embed_dim, seq_len),\n",
    "        strides=(1, embed_dim),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(embed_dim_block, KV_BLOCK_SIZE),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "    v_block_ptr = tl.make_block_ptr(\n",
    "        base=v_ptr + pid_1 * seq_len * embed_dim,\n",
    "        shape=(seq_len, embed_dim),\n",
    "        strides=(embed_dim, 1),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(KV_BLOCK_SIZE, embed_dim_block),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    o_block_ptr = tl.make_block_ptr(\n",
    "        base=o_ptr + pid_1 * seq_len * embed_dim,\n",
    "        shape=(seq_len, embed_dim),\n",
    "        strides=(embed_dim, 1),\n",
    "        offsets=(pid_0 * Q_BLOCK_SIZE, 0),\n",
    "        block_shape=(Q_BLOCK_SIZE, embed_dim_block),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    q = tl.load(q_block_ptr)\n",
    "    attn_norm = attn_norm.to(tl.float16)\n",
    "    q = q * attn_norm\n",
    "    o = tl.zeros((Q_BLOCK_SIZE, embed_dim_block), dtype=tl.float32)\n",
    "    \n",
    "    for i in range(0, seq_len, KV_BLOCK_SIZE):\n",
    "        k = tl.load(k_block_ptr)\n",
    "        v = tl.load(v_block_ptr)\n",
    "        #s = tl.dot(q, k.trans(1, 0)) * attn_norm\n",
    "        s = tl.dot(q, k) \n",
    "        p = 1.0 / (1.0 + tl.exp2(s))  #tl.sigmoid(s)\n",
    "        #print(s.shape)\n",
    "\n",
    "        #v = tl.load(v_ptr + kv_offsets, mask=kv_mask)\n",
    "        #v = tl.load(v_block_ptr)\n",
    "        p = p.to(tl.float16)\n",
    "        o = tl.dot(p, v, o)\n",
    "\n",
    "        #m = mi\n",
    "        k_block_ptr = tl.advance(k_block_ptr, (0, KV_BLOCK_SIZE))\n",
    "        v_block_ptr = tl.advance(v_block_ptr, (KV_BLOCK_SIZE, 0))\n",
    "        \n",
    "    o = o.to(tl.float16)\n",
    "\n",
    "    #o_start = pid_0 * Q_BLOCK_SIZE * embed_dim\n",
    "    #o_offsets = o_start + (tl.arange(0, BATCH_BLOCK_SIZE)[:, None, None] * seq_len * embed_dim + tl.arange(0, Q_BLOCK_SIZE)[None, :, None] * embed_dim + tl.arange(0, EMBED_BLOCK_SIZE)[None, None, :])\n",
    "    #o_mask = (tl.arange(0, BATCH_BLOCK_SIZE)[:, None, None] < batch_size) & (pid_0 * Q_BLOCK_SIZE + tl.arange(0, Q_BLOCK_SIZE)[None, :, None] < seq_len) & (tl.arange(0, EMBED_BLOCK_SIZE)[None, None, :] < embed_dim)\n",
    "    \n",
    "    #tl.store(o_ptr + q_offsets, o, mask=q_mask)\n",
    "    tl.store(o_block_ptr, o)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ea774e-a87a-4a54-9aee-a968af9600cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_flash_sigmoid_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, attn_norm):\n",
    "    batch_size = q.size(0)\n",
    "    num_heads = q.size(1)\n",
    "    seq_len = q.size(2)\n",
    "    embed_dim = q.size(3)\n",
    "    embed_dim_block = triton.next_power_of_2(embed_dim)\n",
    "    if embed_dim == embed_dim_block:\n",
    "        EMBED_DIM_EVEN = True\n",
    "    else:\n",
    "        EMBED_DIM_EVEN = False\n",
    "    batch_size_gemm = batch_size * num_heads\n",
    "    o = torch.empty_like(q)\n",
    "    grid = lambda args: (triton.cdiv(seq_len, args[\"Q_BLOCK_SIZE\"]), batch_size_gemm)\n",
    "    kernel = flash_sigmoid_attention[grid](q, k, v, o, batch_size_gemm, seq_len, attn_norm, embed_dim, embed_dim_block, EMBED_DIM_EVEN)\n",
    "    #print(kernel.asm['ttir'])\n",
    "    return o\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f35a6e2-80a0-4f2e-b262-9f187d710166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Triton autotuning for function flash_sigmoid_attention finished after 3.00s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None;\n",
      "torch.Size([4, 32, 1024, 64])\n",
      "torch.Size([4, 32, 1024, 64])\n",
      "torch.float16\n",
      "torch.float16\n",
      "The maximum difference between torch and triton is 0.0625\n",
      "The mean difference between torch and triton is 0.0033664703369140625\n",
      "tensor([[[[False, False, False,  ..., False,  True, False],\n",
      "          [ True, False, False,  ..., False, False,  True],\n",
      "          [ True,  True, False,  ...,  True, False, False],\n",
      "          ...,\n",
      "          [ True, False,  True,  ..., False, False, False],\n",
      "          [False, False, False,  ..., False, False,  True],\n",
      "          [ True, False, False,  ..., False, False, False]],\n",
      "\n",
      "         [[False,  True,  True,  ...,  True, False, False],\n",
      "          [False,  True,  True,  ...,  True,  True,  True],\n",
      "          [ True,  True,  True,  ..., False,  True,  True],\n",
      "          ...,\n",
      "          [ True,  True,  True,  ...,  True, False,  True],\n",
      "          [False, False,  True,  ...,  True,  True, False],\n",
      "          [False, False,  True,  ..., False,  True,  True]],\n",
      "\n",
      "         [[ True, False,  True,  ..., False, False,  True],\n",
      "          [ True,  True,  True,  ..., False, False, False],\n",
      "          [ True,  True,  True,  ..., False, False,  True],\n",
      "          ...,\n",
      "          [ True, False,  True,  ..., False, False,  True],\n",
      "          [ True,  True,  True,  ..., False, False, False],\n",
      "          [ True, False,  True,  ...,  True, False, False]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[False, False, False,  ..., False,  True, False],\n",
      "          [ True, False, False,  ..., False, False, False],\n",
      "          [False,  True,  True,  ...,  True, False, False],\n",
      "          ...,\n",
      "          [ True, False, False,  ...,  True,  True, False],\n",
      "          [ True, False, False,  ...,  True,  True,  True],\n",
      "          [ True, False,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "         [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "          [ True,  True, False,  ...,  True, False,  True],\n",
      "          [ True,  True,  True,  ..., False,  True,  True],\n",
      "          ...,\n",
      "          [ True,  True, False,  ...,  True,  True,  True],\n",
      "          [ True, False,  True,  ...,  True,  True, False],\n",
      "          [ True, False,  True,  ..., False, False, False]],\n",
      "\n",
      "         [[False,  True,  True,  ...,  True,  True,  True],\n",
      "          [ True,  True,  True,  ...,  True,  True,  True],\n",
      "          [ True, False, False,  ...,  True,  True,  True],\n",
      "          ...,\n",
      "          [False, False,  True,  ..., False,  True, False],\n",
      "          [False,  True,  True,  ...,  True, False, False],\n",
      "          [ True, False,  True,  ..., False,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False, False,  ..., False, False, False],\n",
      "          [False,  True, False,  ...,  True, False, False],\n",
      "          [False, False, False,  ..., False,  True,  True],\n",
      "          ...,\n",
      "          [False, False, False,  ..., False, False,  True],\n",
      "          [ True,  True, False,  ...,  True, False,  True],\n",
      "          [ True,  True, False,  ..., False,  True, False]],\n",
      "\n",
      "         [[ True,  True,  True,  ...,  True, False,  True],\n",
      "          [ True, False, False,  ..., False, False, False],\n",
      "          [False,  True,  True,  ...,  True,  True,  True],\n",
      "          ...,\n",
      "          [ True, False,  True,  ..., False,  True,  True],\n",
      "          [ True,  True,  True,  ..., False, False,  True],\n",
      "          [False,  True, False,  ...,  True, False,  True]],\n",
      "\n",
      "         [[ True,  True, False,  ..., False,  True, False],\n",
      "          [ True, False,  True,  ..., False, False,  True],\n",
      "          [ True, False,  True,  ..., False, False,  True],\n",
      "          ...,\n",
      "          [ True, False,  True,  ..., False, False, False],\n",
      "          [ True, False,  True,  ..., False,  True, False],\n",
      "          [False,  True,  True,  ..., False,  True, False]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[False, False,  True,  ...,  True,  True, False],\n",
      "          [False, False,  True,  ...,  True,  True,  True],\n",
      "          [ True,  True, False,  ..., False,  True, False],\n",
      "          ...,\n",
      "          [ True, False, False,  ...,  True, False,  True],\n",
      "          [False, False,  True,  ...,  True, False,  True],\n",
      "          [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "         [[False, False,  True,  ...,  True, False, False],\n",
      "          [False, False, False,  ..., False,  True, False],\n",
      "          [False, False, False,  ...,  True,  True, False],\n",
      "          ...,\n",
      "          [False, False, False,  ...,  True,  True, False],\n",
      "          [False, False, False,  ...,  True,  True, False],\n",
      "          [False, False,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "         [[ True,  True,  True,  ..., False, False, False],\n",
      "          [ True, False,  True,  ...,  True,  True,  True],\n",
      "          [False, False,  True,  ...,  True,  True,  True],\n",
      "          ...,\n",
      "          [False,  True,  True,  ...,  True, False, False],\n",
      "          [False,  True,  True,  ...,  True, False,  True],\n",
      "          [ True,  True,  True,  ...,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True,  True,  ..., False,  True,  True],\n",
      "          [ True, False, False,  ...,  True,  True,  True],\n",
      "          [False, False,  True,  ...,  True,  True,  True],\n",
      "          ...,\n",
      "          [ True, False, False,  ...,  True, False, False],\n",
      "          [ True,  True, False,  ...,  True,  True, False],\n",
      "          [False,  True, False,  ..., False, False,  True]],\n",
      "\n",
      "         [[False, False,  True,  ...,  True,  True,  True],\n",
      "          [False, False, False,  ...,  True, False, False],\n",
      "          [False, False,  True,  ...,  True,  True,  True],\n",
      "          ...,\n",
      "          [ True, False, False,  ...,  True,  True,  True],\n",
      "          [False,  True,  True,  ..., False,  True,  True],\n",
      "          [ True, False,  True,  ...,  True, False, False]],\n",
      "\n",
      "         [[ True,  True,  True,  ..., False, False, False],\n",
      "          [ True,  True,  True,  ...,  True, False,  True],\n",
      "          [ True,  True,  True,  ..., False, False,  True],\n",
      "          ...,\n",
      "          [ True,  True, False,  ..., False, False,  True],\n",
      "          [ True,  True, False,  ..., False, False,  True],\n",
      "          [ True, False,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[False, False,  True,  ..., False, False, False],\n",
      "          [ True,  True, False,  ...,  True,  True,  True],\n",
      "          [False, False, False,  ..., False,  True, False],\n",
      "          ...,\n",
      "          [False,  True, False,  ...,  True, False, False],\n",
      "          [ True,  True, False,  ..., False, False, False],\n",
      "          [ True, False, False,  ...,  True,  True, False]],\n",
      "\n",
      "         [[ True,  True, False,  ..., False, False,  True],\n",
      "          [ True,  True,  True,  ...,  True, False,  True],\n",
      "          [ True,  True, False,  ..., False, False, False],\n",
      "          ...,\n",
      "          [ True,  True, False,  ..., False, False,  True],\n",
      "          [False,  True, False,  ..., False, False,  True],\n",
      "          [ True,  True, False,  ..., False,  True,  True]],\n",
      "\n",
      "         [[ True,  True, False,  ..., False,  True,  True],\n",
      "          [False,  True,  True,  ..., False, False,  True],\n",
      "          [False, False,  True,  ..., False, False, False],\n",
      "          ...,\n",
      "          [ True,  True,  True,  ...,  True,  True,  True],\n",
      "          [False,  True, False,  ...,  True, False,  True],\n",
      "          [ True,  True, False,  ...,  True, False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True, False, False,  ...,  True, False, False],\n",
      "          [False, False, False,  ..., False, False,  True],\n",
      "          [False,  True,  True,  ...,  True,  True,  True],\n",
      "          ...,\n",
      "          [False,  True,  True,  ..., False,  True,  True],\n",
      "          [False, False, False,  ...,  True, False, False],\n",
      "          [False, False, False,  ..., False,  True,  True]],\n",
      "\n",
      "         [[False,  True,  True,  ..., False,  True,  True],\n",
      "          [ True, False, False,  ...,  True, False,  True],\n",
      "          [False,  True, False,  ...,  True, False,  True],\n",
      "          ...,\n",
      "          [ True, False,  True,  ...,  True,  True,  True],\n",
      "          [ True, False,  True,  ...,  True, False,  True],\n",
      "          [False,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "         [[False, False,  True,  ..., False,  True,  True],\n",
      "          [False, False,  True,  ...,  True,  True,  True],\n",
      "          [False,  True, False,  ...,  True,  True,  True],\n",
      "          ...,\n",
      "          [ True, False, False,  ..., False,  True,  True],\n",
      "          [ True,  True, False,  ...,  True,  True,  True],\n",
      "          [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ True,  True,  True,  ...,  True, False, False],\n",
      "          [False,  True, False,  ..., False,  True, False],\n",
      "          [ True,  True, False,  ...,  True, False, False],\n",
      "          ...,\n",
      "          [False,  True,  True,  ...,  True,  True,  True],\n",
      "          [ True, False, False,  ...,  True,  True, False],\n",
      "          [ True, False, False,  ...,  True,  True, False]],\n",
      "\n",
      "         [[ True,  True, False,  ..., False,  True,  True],\n",
      "          [False, False, False,  ...,  True, False,  True],\n",
      "          [False,  True, False,  ...,  True, False,  True],\n",
      "          ...,\n",
      "          [ True, False,  True,  ...,  True, False,  True],\n",
      "          [False, False, False,  ..., False,  True,  True],\n",
      "          [ True,  True,  True,  ..., False,  True, False]],\n",
      "\n",
      "         [[ True, False, False,  ...,  True,  True,  True],\n",
      "          [ True,  True, False,  ...,  True, False,  True],\n",
      "          [False,  True, False,  ...,  True, False,  True],\n",
      "          ...,\n",
      "          [ True, False, False,  ..., False,  True, False],\n",
      "          [ True, False, False,  ...,  True, False, False],\n",
      "          [ True, False,  True,  ...,  True, False,  True]]]], device='cuda:6')\n",
      "tensor([[ -1.5078,   7.1953,   6.9453,  ...,  -3.2266, -15.6797,   0.5459],\n",
      "        [ -9.3516,  -3.6426,   5.6992,  ...,   6.8398,   3.0918, -11.2656],\n",
      "        [-13.3281,   8.2422,  -1.2344,  ...,  17.0625,   0.1932,  -2.9316],\n",
      "        ...,\n",
      "        [ -6.8047,   0.8789,   7.4648,  ...,  12.7344,   5.3633, -11.5234],\n",
      "        [  0.6558,   2.8965,   0.7637,  ...,   6.9180,   7.9336, -11.0703],\n",
      "        [-15.2188,   2.1836,  -0.4241,  ...,   7.6992,   7.4258,  -7.4805]],\n",
      "       device='cuda:6', dtype=torch.float16)\n",
      "tensor([[ -1.5166,   7.1992,   6.9414,  ...,  -3.2168, -15.6797,   0.5410],\n",
      "        [ -9.3516,  -3.6367,   5.7031,  ...,   6.8359,   3.0938, -11.2656],\n",
      "        [-13.3281,   8.2422,  -1.2334,  ...,  17.0625,   0.1937,  -2.9375],\n",
      "        ...,\n",
      "        [ -6.8047,   0.8809,   7.4648,  ...,  12.7422,   5.3672, -11.5156],\n",
      "        [  0.6509,   2.8945,   0.7646,  ...,   6.9141,   7.9297, -11.0703],\n",
      "        [-15.2188,   2.1855,  -0.4253,  ...,   7.6875,   7.4219,  -7.4766]],\n",
      "       device='cuda:6', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_heads = 32\n",
    "seq_len = 1024\n",
    "embed_dim = 64\n",
    "log2_e = 1.44269504\n",
    "attn_norm = -log2_e / math.sqrt(embed_dim)\n",
    "\n",
    "q = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "k = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "v = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "\n",
    "output_torch = my_scaled_sigmoid_dot_product_attention(q, k, v) #.transpose(0, 1)\n",
    "output_triton = apply_flash_sigmoid_attention(q, k, v, attn_norm) #.transpose(0, 1)\n",
    "print(output_torch.size())\n",
    "print(output_triton.size())\n",
    "print(output_torch.dtype)\n",
    "print(output_triton.dtype)\n",
    "\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(output_torch - output_triton))}')\n",
    "print(f'The mean difference between torch and triton is '\n",
    "      f'{torch.mean(torch.abs(output_torch - output_triton))}')\n",
    "print(output_torch == output_triton)\n",
    "print(output_torch[0, 0])\n",
    "print(output_triton[0, 0])\n",
    "#print(output_torch[0, 2] == output_triton[0, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0611e-6f27-45f6-998b-d587a1cccd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "221ce916-5984-437b-bc43-07b527c194ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['size'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range (9, 15, 1)],  # Different possible values for `x_name`.\n",
    "        #x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['flash', 'triton_softmax', 'triton_sigmoid'],  # Possible values for `line_arg`.\n",
    "        line_names=['Flash', 'Triton softmax', 'Triton sigmoid'],  # Label name for the lines.\n",
    "        styles=[('green', '-'), ('blue', '-'), ('red', '-')],  # Line styles.\n",
    "        ylabel='TFLOPS',  # Label name for the y-axis.\n",
    "        plot_name='flash-attn-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(size, provider):\n",
    "    batch_size = 4\n",
    "    num_heads = 32\n",
    "    seq_len = size\n",
    "    embed_dim = 128\n",
    "    log2_e = 1.44269504\n",
    "    attn_norm = log2_e / math.sqrt(embed_dim)\n",
    "    if provider == 'flash':\n",
    "        qkv = torch.randn((batch_size, seq_len, 3, num_heads, embed_dim), dtype=torch.float16, device=device)\n",
    "        fn = lambda: flash_attn_func(qkv)\n",
    "        ms = triton.testing.do_bench(fn)\n",
    "    if provider == 'triton_softmax':\n",
    "        q = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "        k = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "        v = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "        ms = triton.testing.do_bench(lambda: apply_flash_attention(q, k, v, attn_norm))\n",
    "    if provider == 'triton_sigmoid':\n",
    "        attn_norm_sigmoid = - attn_norm\n",
    "        q = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "        k = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "        v = torch.randn((batch_size, num_heads, seq_len, embed_dim), device=device, dtype=torch.float16)\n",
    "        ms = triton.testing.do_bench(lambda: apply_flash_sigmoid_attention(q, k, v, attn_norm_sigmoid))\n",
    "\n",
    "    flops_per_matmul = 2.0 * batch_size * num_heads * seq_len * seq_len * embed_dim\n",
    "    total_flops = 2 * flops_per_matmul\n",
    "    return total_flops * 1e-12 / (ms * 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df0e0a8e-6901-4010-bfe8-a50c6959c007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_attention finished after 2.48s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None;\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_sigmoid_attention finished after 2.47s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None;\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_attention finished after 2.54s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None;\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_sigmoid_attention finished after 2.52s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None;\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_attention finished after 2.76s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None;\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_sigmoid_attention finished after 2.72s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None;\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_attention finished after 3.57s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None;\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_sigmoid_attention finished after 3.41s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None;\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_attention finished after 7.50s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None;\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_sigmoid_attention finished after 6.56s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None;\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_attention finished after 26.96s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None;\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 64, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 221184, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 3, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 4, maxnreg: None\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 4, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Autotuning kernel flash_sigmoid_attention with config Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 64, num_warps: 8, num_ctas: 1, num_stages: 7, maxnreg: None\n",
      "Autotuning failed with out of resource: shared memory, Required: 229376, Hardware limit: 166912. Reducing block sizes or `num_stages` may help.\n",
      "Triton autotuning for function flash_sigmoid_attention finished after 22.39s; best config selected: Q_BLOCK_SIZE: 128, KV_BLOCK_SIZE: 32, num_warps: 4, num_ctas: 1, num_stages: 3, maxnreg: None;\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGxCAYAAACEFXd4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABYXklEQVR4nO3deXhTVf4G8DdN9y2lpSulBZGtgICAWBAEqRQFlWUUnYpFWcRfQVkHEEEUFUZxAUTQmREYBwVRNkE2oRRZBAFZylIW2aEtdElX2rQ5vz+uuTR0IW2T3CR9P8+Tp8m9Nzffw9TmnXPPOVclhBAgIiIiclBOShdAREREZEkMO0REROTQGHaIiIjIoTHsEBERkUNj2CEiIiKHxrBDREREDo1hh4iIiBwaww4RERE5NGelC7AFer0e169fh4+PD1QqldLlEBERkQmEEMjNzUVYWBicnCrvv2HYAXD9+nU0bNhQ6TKIiIioBq5cuYLw8PBK9ysadmbPno3Vq1fj9OnT8PDwQJcuXfDPf/4TzZs3BwBkZmbi7bffxtatW3H58mUEBgaif//+mDVrFjQajXyey5cv47XXXkNiYiK8vb0RHx+P2bNnw9nZtOb5+PgAkP6xfH19zd9QIiIiMrucnBw0bNhQ/h6vjKJhJykpCQkJCejUqRNKSkrw5ptvonfv3jh58iS8vLxw/fp1XL9+HXPnzkVUVBQuXbqEUaNG4fr16/jhhx8AAKWlpejbty9CQkKwd+9e3LhxAy+99BJcXFzwwQcfmFSH4dKVr68vww4REZGdudcQFJUt3Qj05s2bCAoKQlJSErp3717hMatWrcKLL76I/Px8ODs7Y9OmTejXrx+uX7+O4OBgAMDixYsxefJk3Lx5E66urvf83JycHGg0Gmi1WoYdIiIiO2Hq97dNzcbSarUAAH9//yqP8fX1lS9R7du3D23atJGDDgDExsYiJycHJ06cqPAcRUVFyMnJMXoQERGRY7KZsKPX6zF27Fh07doVrVu3rvCYW7duYdasWRg5cqS8LTU11SjoAJBfp6amVnie2bNnQ6PRyA8OTiYiInJcNhN2EhISkJycjBUrVlS4PycnB3379kVUVBRmzpxZq8+aOnUqtFqt/Lhy5UqtzkdERES2yyamno8ePRobNmzArl27Kpw6lpubiz59+sDHxwdr1qyBi4uLvC8kJAQHDhwwOj4tLU3eVxE3Nze4ubmZsQVERERkqxTt2RFCYPTo0VizZg127NiBxo0blzsmJycHvXv3hqurK9avXw93d3ej/dHR0Th+/DjS09Plbdu2bYOvry+ioqIs3gYiIiKybYr27CQkJODbb7/FunXr4OPjI4+x0Wg08PDwkINOQUEB/ve//xkNJg4MDIRarUbv3r0RFRWFIUOG4MMPP0RqaireeustJCQksPeGiIiIlJ16Xtm8+CVLlmDo0KHYuXMnevbsWeExFy5cQKNGjQAAly5dwmuvvYadO3fCy8sL8fHxmDNnjsmLCnLqORERkf0x9fvbptbZUQrDDhERkf2xy3V2iIiIiMyNYYeIiIgcGsMOEREROTSbWGeHiIiI7JcQAnqhR4m+BCX6Euj0Ovm54RHsFQwPFw9F6mPYISIiMqO7v/gr+/LXlVawzRrHWegz7mXbkG2IuS/GCv8LlMewQ0REFlWqL1XmS13BL3+SOKmc4OzkDGcnZyg5+Zthh4jIivRCr9yXukJf/gJ1foUTAMZf/IaHi5NL+W3qCrbd6zhVLd5roeOcnZzhpLKNocEMO0Rkd4QQ0BZpcTP/Jm4W3ERmYSaKS4vt4sufX/wSR/7ir+xYW/nir4sYdohIcXqhR1ZhFtLz03Gz4KYcYuSfd227VXDLoS4VOMoXf3WO5Rc/WRPDDhGZXYm+BBkFGRUHlwoCTEZBBkpFabU/x8fVB4FegQjwCICbs5tiX/y1PSe/+Iksi2GHiO6puLS48tBSQXjJKsyq0eUaP3c/BHoGItArEIGegQjyCjJ6XfZnfc/6cHd2t0BricjRMOwQ1UGFusJKg4t8KanM9pyinGp/hgoqBHgGGIeUSoJLoKcUXlzULhZoLRHVdQw7RHZOCIG84jyTe11u5t9Evi6/2p+jVqlR37O+UUgJ8gyqNLz4e/hD7aS2QIuJiKqHYYfIxpSdaWTqgN2i0qJqf46r2rXinpZKel/83P04toSI7BLDDpGF6YUemYWZJve61HSmkYezR4XBJcir4t4XH1cfqFQqC7SYiMi2MOwQVVOJvgS3Cm6ZHF4yCjOgF/pqf45hppGpY168XL0s0FoiIvvHsEN1nikzjcpeTsq6nVWjz7l7ppE87qWCGUecaUREZD4MO+TwcopycCbjDFJupSAlQ3pc1l7mTCMiojqCYYccQom+BBeyLkhh5laKFG7+Cjapean3fH9FM42qWueFM42IiOwHww7ZDSEEbhbcLBdmUm6l4HzWeZToSyp9b7BXMJrXb47mAdKjcb3GRkGGM42IiBwXww7ZnEJdIc5lnrvTS5N55xJU9u3sSt/n4eyBZgHN0CygmRRq/go3zQKaQeOusV4DiIjIpjDskCL0Qo+rOVfLjaVJuSWNp6nsVgMqqBChiTDqpWkW0AzN6zdHuG84e2eIiKgchh2yqJyinDthpkwvzZmMMygsKaz0fX7ufneCTJlemvv974eHi4cVW0BERPaOYYdqTVeqw4XsCxX20qTlp1X6PmcnZzSp16TCXppAz0AueEdERGbBsEMmKTs4+O5emnsNDg7xDikXZgyDhJ2d+CtIRESWxW8aMlKoK8TZzLPlemnOZJwxaXDw3b00HBxMRERKY9ghAMDtktt4dtWz2HhmY5WDgyP9IiscS9PAtwEHBxMRkU1i2CEAwLzf5mHDmQ0A7gwOvruXhoODiYjIHjHsEFLzUvH+r+8DAL5++msMbTeUg4OJiMhhMOwQpu+YjtziXHQK64T4dvEMOkREjk4IoLQUKCkBdLo7j7Kvq9pXk2OHDgWaNFGkuQw7ddyR1CP4zx//AQB81uczjrshIirLEArM9YVfk2Mt9TnW1q0bww5ZnxACYzePhYDA862fR5eGXZQuiYjslRDSF6kSX9qW/py6xNkZcHGRHmWfV/d1RfvCw5VrlmKfTIpbe3otki4lwd3ZHXN6zVG6HKK6wRAKlPx/9Jb4nLoWCkz9gq9OGFD6vWo14KDDGBh26qiikiJM3DYRADAxeiIi/SIVroiomoQAtFogIwPIzgaKiuwjHJSWKv0vZ1229oVujvc6cChwVAw7ddT8/fPxZ9afCPUOxeRHJitdDtV1xcVAZqYUXCp63LpVfltmpuMEB5XK9r/ga3KsWq30vywRAIadOiktLw2zds0CAMzuNRvert4KV0QOQwggN7fy0FLZIze35p/p6QnUqwe4u9tGGKhpTwERWQzDTh00I3EGcotz0SG0A4a0HaJ0OWSrSkqq7m2pqNclM7PmszxUKim0BAQA9etLP015uLubt91E5HAYduqYo6lH8e8//g2AU83rDCGA/Pzq97ZotTX/THd308OK4eHnxx4OIrIIhp06RAiBcVvGQS/0eK7Vc3gk4hGlS6LqKi0FsrKq19uSkSGNiakpPz/jUGJKr4unp9maTERUWww7dcj6lPVIvJgIN7Ub/hnzT6XLoYKC6ve2ZGdLPTU14epa/d6WevWkMSVERHaMf8XqiKKSIkzYOgEAMCF6Ahr5NVK2IEei10shxNTAYuh1uX275p/p61v93hYvL06XJaI6iWGnjvj8wOc4n3UeId4hmPLIFKXLsV23b1e/tyUrSwo8NeHsXP3eFn9/aRYPERGZhGGnDriZfxPv7noXAPDBYx/Ax81H4YoU9OefwJYtQHJyxb0tBQU1P7e3d+UBpbKeFx8f9rYQEVkYw04dMCNxBnKKctA+pD3i28UrXY515ecDSUnA5s3S4+zZe79HrZZ6T6rb2+LmZvn2EBFRtTHsOLjjacfx1eGvANSRqeZCACdOSL03mzcDu3YZz0Rydga6dAG6dgWCgirudfH1BZwc/N+JiKgOYdhxYGWnmg9qOQjdI7srXZJlZGUB27ff6b25ds14f2Qk0KeP9HjsMSnMEBFRncGw48A2nNmA7Re2w1Xtig8f/1DpcsyntBQ4dOhO781vvxkPEHZ3B3r0uBNwmjXjuBgiojqMYcdBFZcWy1PNxz08DvfVu0/himopNRXYulUKN1u3SoOJy4qKAmJjpXDTrRvg4aFMnUREZHMYdhzUwgMLcTbzLIK8gvBmtzeVLqf6iouBvXulcLNlC3DkiPF+X1/g8celgBMbC0REKFImERHZPoYdB3Sr4BbeSXoHAPD+Y+/D181OxqhcuHBn3M2OHUBenvH+jh3v9N507sy1ZoiIyCQMOw5o5s6Z0BZp0Ta4LV5u97LS5VSuoADYufNO782ZM8b7g4Lu9Nw8/rj0moiIqJoYdhzMifQTWHxwMQDg09hPoXayobtICwGcPHmn9+bXX4Giojv7DdPC+/SRAk67dpwCTkREtcaw40CEEBi/dTxKRSkGtBiAno17Kl2SdM+oX36503tz9arxfk4LJyIiC2PYcSCbzm3C1vNb4eLkgo8e/0iZIvR6aVq4ofdm/35pqrgBp4UTEZGVMew4CF2pDuO3jAcAjH14LJr4N7Heh99rWnjLlnfCDaeFExGRlTHsOIhFBxchJSMFgZ6BmNZtmmU/rLgY2LfvTu9NRdPCY2LujL3htHAiIlIQw44DyCjIwMydMwEA7z32HjTuGvN/yIULd1Ys3r69/LTwDh3u9N5wWjgREdkQhh0H8E7SO8i6nYUHgh/AsPbDzHNSw7RwQ8CpaFp4795SuOG0cCIismEMO3bu1M1T+OL3LwCYYar5lSvAqlV37hZedlq4Wi3dKdywqB+nhRMRkZ1g2LFzE7ZOQKkoxTPNn8FjjR+r+YnS0oD27Y0HF0dEGE8L11jg8hgREZGFMezYsU1nN2HTuU3mmWq+cKEUdO67DxgzRgo4zZtzWjgREdk9hh07pSvVyXc1f73z62ga0LTmJysoAL6QLoXhww+BQYPMUCEREZFt4KALO/XloS9x6tYp1Pesj7e6v1W7ky1bdqdXp39/s9RHRERkKxh27FBmYSbe3vk2AGBWz1nwc/er+clKS4FPPpGejxsnDUQmIiJyIIqGndmzZ6NTp07w8fFBUFAQ+vfvj5SUFKNjbt++jYSEBAQEBMDb2xuDBg1CWlqa0TGXL19G37594enpiaCgIEyaNAklJSXWbIpVvZv0LjILM9E6qDWGPzi8didbvx44dw6oVw942YbvkE5ERFRDioadpKQkJCQk4LfffsO2bdug0+nQu3dv5Ofny8eMGzcOP/30E1atWoWkpCRcv34dAwcOlPeXlpaib9++KC4uxt69e7Fs2TIsXboUM2bMUKJJFnf61mks/H0hAOCT3p/A2amWw64+/lj6+dprgJdXLasjIiKyPSohhFC6CIObN28iKCgISUlJ6N69O7RaLQIDA/Htt9/ib3/7GwDg9OnTaNmyJfbt24eHH34YmzZtQr9+/XD9+nUEBwcDABYvXozJkyfj5s2bcHV1vefn5uTkQKPRQKvVwtfG77rd79t+2Hh2I55q9hTWv7C+difbtw/o0gVwdQUuXgRCQ81SIxERkTWY+v1tU2N2tFotAMDf3x8AcOjQIeh0OsTExMjHtGjRAhEREdi3bx8AYN++fWjTpo0cdAAgNjYWOTk5OHHiRIWfU1RUhJycHKOHPdhybgs2nt0IZydnzO09t/YnNPTqvPgigw4RETksmwk7er0eY8eORdeuXdG6dWsAQGpqKlxdXeHn52d0bHBwMFJTU+VjygYdw37DvorMnj0bGo1GfjRs2NDMrTG/En0Jxm+V7mo+5qExaBbQrHYnPH8eWLNGej5+fC2rIyIisl02E3YSEhKQnJyMFStWWPyzpk6dCq1WKz+uXLli8c+sra8OfYWTN08iwCMA07tPr/0JP/sM0OuBJ54AWrWq/fmIiIhslE0sKjh69Ghs2LABu3btQnh4uLw9JCQExcXFyM7ONurdSUtLQ0hIiHzMgQMHjM5nmK1lOOZubm5ucHNzM3MrLCerMAszEqUB1+/2fBf1POrV7oQZGcDXX0vPJ06sZXVERES2TdGeHSEERo8ejTVr1mDHjh1o3Lix0f4OHTrAxcUF27dvl7elpKTg8uXLiI6OBgBER0fj+PHjSE9Pl4/Ztm0bfH19ERUVZZ2GWNisXbOQUZiBqMAojOwwsvYnXLxYWjW5fXugZ8/an4+IiMiGKdqzk5CQgG+//Rbr1q2Dj4+PPMZGo9HAw8MDGo0Gw4YNw/jx4+Hv7w9fX1+MGTMG0dHRePjhhwEAvXv3RlRUFIYMGYIPP/wQqampeOutt5CQkGBXvTeVOZNxBgsOLAAg3dW81lPNi4qABdL5MGEC731FREQOT9Gws2jRIgBAjx49jLYvWbIEQ4cOBQB8+umncHJywqBBg1BUVITY2Fh8YbiPEwC1Wo0NGzbgtddeQ3R0NLy8vBAfH493333XWs2wqIlbJ6JEX4K+Tfuid5PetT/h8uXSHc7Dw4Hnnqv9+YiIiGycTa2zoxRbXWdn2/lt6P2/3nB2csbx146jRf0WtTuhXg+0bg2cOgXMnSv17BAREdkpu1xnh+4oO9U8oVNC7YMOAGzeLAUdX19gxIjan4+IiMgOMOzYqH8f/jeS05Ph7+GPGY+a6dYXhkUER4yQAg8REVEdwLBjg/KL8zE9UVpL550e78Dfw7/2Jz18GNixA3B2Bt54o/bnIyIishMMOzboWNox3Cq4hWCvYLza4VXznNTQqzN4MGAHK0YTERGZC8OODbqacxUA0MS/CVzULrU/4ZUrwMqV0nMOSiYiojqGYccGGcJOuG/4PY400bx5QGkp8Nhj0kKCREREdQjDjg26lnsNABDuY4awo9UCX30lPeetIYiIqA5i2LFBZu3Z+de/gNxcICoK6NOn9ucjIiKyMww7NsgQdhr4NqjdiXQ66RIWwFtDEBFRncWwY4Pky1i17dn5/nvg6lUgOBiIizNDZURERPaHYcfG6IUe13LMEHaEkG4JAQCvvw44wE1RiYiIaoJhx8bczL8JnV4HFVQI9Q6t+YkSE4EjRwBPT2DUKLPVR0REZG8YdmyM4RJWsHdw7dbYMfTqvPIK4G+GFZiJiIjsFMOOjTHLTKzkZGDTJsDJCRg71jyFERER2SmGHRsjz8TyqcVMrE8+kX4OHAg0aWKGqoiIiOwXw46NqfXg5Bs3gOXLpee8NQQRERHDjq25mlvLy1iffw4UFwNduwIPP2zGyoiIiOwTw46NqdVlrLw8YNEi6TlvDUFERASAYcfm1Ooy1pIlQFYWcP/9wFNPmbkyIiIi+8SwY0OEEDWfjVVaCnz6qfR8/HhArTZzdURERPaJYceGaIu0yNflA6jBfbHWrAEuXAACAoD4eAtUR0REZJ8YdmyI4RJWPfd68HTxNP2NZW8NkZAgrZpMREREABh2bEqNL2Ht3Qvs3y/d/yohwQKVERER2S+GHRtS47Bj6NV56SUgKMjMVREREdk3hh0bUqNp52fOAOvWSc/Hj7dAVURERPaNYceGGG4CWq2enU8/lcbs9OsHtGhhocqIiIjsl7PSBdAd1b6MdfMmsHSp9JyLCJIDy88Hrl4Frl2Tft79PDVVWn3B0QmhdAXWU1faWlfaCQDffw/07KnMZzPs2BD5Mpap084XLQJu3wY6dgS6d7dgZUSWIQSQnV1xgCn7Ojtb6UqJqLZ0OuU+m2HHhlTrMlZhoXQfLEDq1VGpLFgZUfXp9UB6etU9MlevSr/KpvD2BsLD7zwaNLjzPDQUcHGxbHtsRV36T72utLWutDMyUrnPZtixEQW6AmQWZgIwMex88410GSsyEhg0yMLVERnT6YAbN6rukbl2DSgpMe18AQEVh5iyr319LdsmInJcDDs2wrCgoKeLJzRumqoP1uuBTz6Rno8dCzjzf0Yyn4KCO+Glsh6ZtDTTxhqoVFKvy90hpuzzsDDAw8Py7SKiuovfkjai7CUs1b36NDduBFJSAI0GGDbMCtWRIxAC0GqrvqR07RqQmWna+VxcKu+FMTwPCak7l5eIyHYx7NiIas3EMiwi+OqrgI+PBasie6HXA7duVRxiyj7PzzftfF5e976sVL8+4MTFK4jIDjDs2AiTFxT8/Xdg1y7p0tXrr1uhMlJaSYk0PqaqHpnr14HiYtPO5+9fdY+MYXxMXRk0SUSOj2HHRhjG7NyzZ+fjj6Wff/+79A1Fdu327cpDjOF5aqrUc3MvKhUQHFx1j0yDBrxPLBHVPQw7NuJqrgmXsS5eBFatkp5PmGD5oqhWcnLuPe06I8O0czk73wkvlQ30rUvTr4mIqoNhx0aYNGbns8+k/4v/+OPAAw9YpzAqRwgppFS0+F3Z57m5pp3Pw8O4B6aiQBMUxPExREQ1xbBjIwyXsSods5OVBfz739Jz3hrCYkpLpctGVfXIXLsGFBWZdj4/v3sP9PXz4/gYIiJLYtixAbpSHVLzUgFU0bPz1VfSVJo2baSeHaqxa9ekMd4VTbu+ccP0eywZxsdUdlmpQQNpVhMRESmLYccG3Mi7AQEBFycXBHoFlj+guBiYN096PmECuwFqqLhYGt/97rvSwODKqNXSQndVXVYKCwNcXa1XOxER1RzDjg0wXMIK8wmDk6qCgRnffSd1OYSFAS+8YOXqHENSEvDaa8CpU9Lrdu2AVq0qDjRBQVLgISIix8CwYwOqHJwsxJ3p5q+/zu6Earp5E5g0CVi2THodFCTdaePvf2cHGRFRXcH5HTagyrCzbRtw/Lg0+GPkSCtXZr/0emk8d/PmUtBRqYBRo4DTp4G4OAYdIqK6hD07NqDK1ZMNt4YYPhyoV8+KVdmv48elYLN3r/S6bVtg8WLg4YeVrYuIiJTBnh0bUPYmoEaOHZN6dpycpLubU5Xy8qRLVu3bS0HH2xv49FPg4EEGHSKiuow9Ozag0stYhrE6zz4LNGpk3aLszLp1wJgxwJUr0utBg6Q1GMNNuK8qERE5NoYdGyBfxvItcxnr6lXg22+l57w1RKUuXZLGba9fL71u1Aj4/HOgb19FyyIiIhvCy1gK0ws9rudeB3BXz86CBdLtrrt3Bzp1Uqg626XTAR99BERFSUHHxQWYOhU4cYJBh4iIjLFnR2E3829Cp9dBBRVCvUOljfn5wJdfSs95a4hy9uyRBiAnJ0uvu3cHFi2Sgg8REdHd2LOjMMMlrGDvYLio/7pl9bFjgFYr3caa3RSyjAxpUtojj0hBp359YOlSYOdOBh0iIqoce3YUVuFMrMuXpZ9Nm/JW15DWVVy2TOrkysiQtg0fDsyZAwQEKFsbERHZPoYdhVU4E8sQdiIiFKjItpw8Kd3mYdcu6XXr1tKaOV27KlsXERHZD3YbKEwOOz4MO2UVFABvviktCLhrF+DpCXz4IXD4MIMOERFVD3t2FGa4jGU07byOh52NG4HRo4GLF6XXzzwDzJ9fZ/85iIioltizozBexrrj6lVpMcB+/aSg07AhsHat9Khj/xRERGRGDDsKqzDsXLok/awj3/AlJdJtHVq2BFavBtRq6bYPJ09KvTpERES1wctYChJC4FrOX5exDDcBzc0FsrKk5w0bKlSZ9fz2m7RmztGj0usuXaQByG3aKFsXERE5DvbsKEhbpEW+Lh9AmTE7hps7+fkBvr7KFGYFWVlSyOnSRQo6/v7Av/8N/Porgw4REZkXe3YUZLiE5e/hD08XT2mjg4/XEQJYvly63Vd6urRt6FBpplVgoKKlERGRg2LYUVC5S1jAnbATGalARZZ1+jTwf/8HJCZKr1u2lG7z8OijytZFRESOjZexFFRXZmIVFgLTpwMPPCAFHQ8PYPZs4MgRBh0iIrI89uwoqC6EnS1bpN6cP/+UXj/5JPD550DjxsrWRUREdQd7dhRkCDsVXsay87Bz/ToweDDQp48UdBo0AH78EdiwgUGHiIisi2FHQVXeBNROw05pqbTacYsWwPffS2vmjBsHnDoFDBwIqFRKV0hERHWNomFn165deOqppxAWFgaVSoW1a9ca7c/Ly8Po0aMRHh4ODw8PREVFYfHixUbH3L59GwkJCQgICIC3tzcGDRqEtLQ0K7ai5spdxiotlZYRBuwy7Pz+O/DQQ8Abb0jLBXXuDBw8CHzyCeDjo3R1RERUVykadvLz89G2bVssXLiwwv3jx4/H5s2b8b///Q+nTp3C2LFjMXr0aKxfv14+Zty4cfjpp5+watUqJCUl4fr16xg4cKC1mlAr5cJOaiqg00ndIaGhClZWPVqtdC+rzp2lG3X6+UkLA+7dC7Rrp3R1RERU1yk6QPmJJ57AE088Uen+vXv3Ij4+Hj169AAAjBw5El9++SUOHDiAp59+GlqtFv/5z3/w7bff4rHHHgMALFmyBC1btsRvv/2Ghx9+uMLzFhUVoaioSH6dk5NjvkaZqEBXgKzb0krJ8oKChktY4eFS4LFxQgArV0qXqVJTpW0vvgjMnQsEBytbGxERkYFNj9np0qUL1q9fj2vXrkEIgcTERJw5cwa9e/cGABw6dAg6nQ4xMTHye1q0aIGIiAjs27ev0vPOnj0bGo1GfjRU4LYMhjV2vFy8oHHTSBvtaLzO2bNA797ACy9IQadZM2D7duCbbxh0iIjItth02FmwYAGioqIQHh4OV1dX9OnTBwsXLkT37t0BAKmpqXB1dYWfn5/R+4KDg5Fq6GqowNSpU6HVauXHFcMtGqyo7CUslWHUrh0sKHj7NvDOO9ItHX75BXBzA959Fzh2DPirc42IiMim2PQ6OwsWLMBvv/2G9evXIzIyErt27UJCQgLCwsKMenOqy83NDW5ubmastPoMM7HkS1iAzffs/PKLtGbO2bPS6969gYULgfvvV7YuIiKiqths2CksLMSbb76JNWvWoG/fvgCABx54AEeOHMHcuXMRExODkJAQFBcXIzs726h3Jy0tDSEhIQpVbhp7WlAwNVW6l9W330qvQ0OBzz4Dnn2WU8mJiMj22exlLJ1OB51OBycn4xLVajX0ej0AoEOHDnBxccH27dvl/SkpKbh8+TKio6OtWm91yWHHx3bDTmmpdO+qFi2koOPkBIwZI62Z89xzDDpERGQfFO3ZycvLw7lz5+TXFy5cwJEjR+Dv74+IiAg8+uijmDRpEjw8PBAZGYmkpCT897//xSeffAIA0Gg0GDZsGMaPHw9/f3/4+vpizJgxiI6OrnQmlq2w9ctYhw8Do0ZJa+cAQIcO0nTyjh2VrYuIiKi6FA07Bw8eRM+ePeXX48ePBwDEx8dj6dKlWLFiBaZOnYq4uDhkZmYiMjIS77//PkaNGiW/59NPP4WTkxMGDRqEoqIixMbG4osvvrB6W6qr3GWsvDwgM1N6rsDsMIOcHGDGDGDBAkCvB3x9gQ8+kIKPHcyGJyIiKkclhBCmHLhv3z5kZGSgX79+8rb//ve/ePvtt5Gfn4/+/ftjwYIFig/8rYmcnBxoNBpotVr4+vpa5TNDPw5Fal4qDo08hAdDH5SuDUVFSSvyZWVZpYa7rV0LJCRI97UCgOefl1Y/tqP1DYmIqA4x9fvb5DE77777Lk6cOCG/Pn78OIYNG4aYmBhMmTIFP/30E2bPnl27qusIXakOaXnSLS3km4BeuiT9VOgS1oYN0r2rrl8HmjSR7lb+3XcMOkREZP9MDjtHjhxBr1695NcrVqxA586d8a9//Qvjx4/H/Pnz8f3331ukSEdzI+8GBARcnFwQ6BUobVRwvE5KChAXJ62IPHQokJwsTSsnIiJyBCaP2cnKykJwmaVxk5KSjG710KlTJ0UW57NHhvE6DXwbwEn1V95UKOzk5AD9+0s/H3kE+PJLwNXVqiUQERFZlMk9O8HBwbhw4QIAoLi4GIcPHzaa8ZSbmwsXFxfzV+iADLeKkC9hAYqsnqzXA/HxwOnTQFgYsGoVgw4RETkek8POk08+iSlTpuDXX3/F1KlT4enpiW7dusn7jx07hiZNmlikSEdjKwsKfvCBNCjZ1RVYvRqw8XUYiYiIasTky1izZs3CwIED8eijj8Lb2xtLly6Fa5lugK+//lq+QSdVzRbCzsaN0hRzAPjiC6BzZ6t8LBERkdWZHHbq16+PXbt2QavVwtvbG+q7Fl1ZtWoVvL29zV6gI7qae1fYKS0FrkrbrBF2zpwB/v53aUDyqFHAsGEW/0giIiLFVGtRwYsXL2Lbtm3Q6XTo3r07WrduLe/z9/c3e3GOqtyYnbQ0QKeTVu2z8Fzv3FxgwABpQHKXLsC8eRb9OCIiIsWZHHYSExPRr18/FBYWSm90dsbXX3+NF1980WLFOapyl7EMl7DCwy26TLFhQPLJk9KA5B9+4IBkIiJyfCYPUJ4+fToef/xxXLt2DRkZGRgxYgT+8Y9/WLI2h6QXevm+WHLYsdKCgrNnA2vWSAHnxx+5YCAREdUNJoed5ORkfPDBBwgNDUW9evXw0UcfIT09HRkZGZasz+HczL+JEn0JVFAhxPuv6U9WGJy8cSMwfbr0fOFCwMbvk0pERGQ2JoednJwc1K9fX37t6ekJDw8PaLVaixTmqAyXsEK8Q+Ci/mtdIguHnbNn76yQ/OqrwPDhFvkYIiIim1StAcpbtmyBRqORX+v1emzfvh3Jycnytqefftp81Tkga087z82VVkjWaqUByfPnm/0jiIiIbFq1wk58fHy5ba+++qr8XKVSobS0tPZVOTDDeJ0GvpZfPdlwr6uTJ6XxORyQTEREdZHJYUev11uyjjpD7tnxsXzPzuzZ0srILi4ckExERHWXyWN2yDzKXcbKywMyM6XnDRua7XM2bQLeekt6vnAhEB1ttlMTERHZlWqHnVWrVmHgwIFo3bo1WrdujYEDB+KHH36wRG0OqdxlLMOd4v38AF9fs3zGuXN3VkgeORIYMcIspyUiIrJLJocdvV6PwYMHY/DgwTh58iTuv/9+3H///Thx4gQGDx6M559/HkIIS9bqECpdUNBMl7Dy8qQBydnZUm8OByQTEVFdZ/KYnXnz5uGXX37B+vXr0a9fP6N969evx8svv4x58+Zh7Nix5q7RYQghLBp2hABefhk4cUK6g/kPPwBubrU+LRERkV0zuWdnyZIl+Oijj8oFHUCabv7hhx/i66+/NmtxjkZbpEWBrgBAmftimXH15DlzpIBjGJAcFlbrUxIREdk9k8PO2bNnERMTU+n+mJgYnD171ixFOSpDr46/hz88XDykjWbq2dm8GZg2TXq+YIG0pg4RERFVI+x4eHggOzu70v05OTlwd3c3R00Oy1ILCp47B7zwgnQZa8QIaZVkIiIikpgcdqKjo7Fo0aJK9y9cuBDRnN9cpWs5d90AFKj1goJ5ecCAAdKA5Icflnp1iIiI6A6TByhPmzYNPXr0QEZGBiZOnIgWLVpACIFTp07h448/xrp165CYmGjJWu2eoWdHHq9TWgpclbbVpGfHMCA5OVkakPzjjxyQTEREdDeTw06XLl2wcuVKjBw5Ej/++KPRvnr16uG7775D165dzV6gIyl3GSstDdDpALW6Rssbf/jhnQHJP/zAAclEREQVqda9sQYMGIDY2Fhs2bJFHozcrFkz9O7dG66urrh+/TrC+I1bKcOCguWmnYeHS4GnGrZsAaZOlZ7Pnw8wZxIREVWsWmEHADw9PTFgwIBy248ePYoHH3yQNwKtQrnLWDUcnHz+PPD889JlrOHDOSCZiIioKrw3lhWZY0HB/Pw7A5I7dwY+/xxQqcxcKBERkQNh2LGSAl0Bsm5nASgTdqq5oKAQwCuvAMePc0AyERGRqRh2rMQw7dzLxQu+bn/d8LOaPTsffQR8/z3g7CwNSG7QwBKVEhERORaTx+wcO3asyv0pKSm1LsaRlb2EpTJcd6pG2Nm6lQOSiYiIasLksNOuXTuoVKoK72xu2K7i4JFK1Wb15D//lAYk6/XAsGHAqFGWqpKIiMjxmBx2Lly4YMk6HJ5h2nkD37+uPeXlAZmZ0vMqwk5+PtC/P5CVBTz0EAckExERVZfJYSeyhrczIIncs+PzV8/OlSvSTz8/wNe3wvcIIfXkHD8OBAcDq1cDvP0YERFR9Zg8QPmll15Cbm6u/Pro0aPQ6XQWKcoR1WTa+dy5wMqVHJBMRERUGyaHneXLl6OwsFB+3a1bN1wx9E7QPVW6enIlYWfbNmDKFOn5vHnAI49YukIiIiLHZHLYuXtgckUDlaly8urJvvdePTk3986A5FdeAV57zVpVEhEROR6us2MFulId0vLSAJjWs3P8uDR2OTgYWLiQA5KJiIhqo1r3xjp58iRSU1MBSD07p0+fRl5entExDzzwgPmqcxA38m5AQMBV7Yr6nvWljVWsnvzXPzHuu48DkomIiGqrWmGnV69eRpev+vXrB8B4nR3eCLQ8wyWsMJ8wOKn+6kyromfHEHZCQ61RHRERkWPjOjtWUG4mVmkpcFXaVlXYCQmxRnVERESOzeSws2zZMkycOBGenp6WrMchGe6LJYedtDRApwPU6gq7bxh2iIiIzMfkAcrvvPNOufE5ZBp5JpbPXTOxwsOlRXTucuOG9JNhh4iIqPZqPPWcTHc1t3oLCrJnh4iIyHyqNfWcN/qsmXKXsUwMOxygTEREVHvVmo3VrFmzewaeTMPNLUlW6WWsCsKOXi8N6QHYs0NERGQO1Qo777zzDjQajaVqcUh6oa/WrSKysqSxywAQFGSNComIiBxbtcLO888/jyB+A1dLen46SvQlcFI5IcT7r66aKsKOYXByQADg6mqlIomIiByYyWN2OF6nZgzjdUK8Q+CidpE2mrB6Mi9hERERmQdnY1lYufE6eXnSja8Arp5MRERkBSZfxtLr9Zasw2GVWz35yhXpp0YD+PqWO549O0RERObFu55bWKWDkyMjKzyeYYeIiMi8GHYsrDrTzgGunkxERGRuDDsWVu4yFldPJiIisiqGHQurzho7AMMOERGRuTHsWJAQ4s5lLF/TLmNxNhYREZF5MexYUPbtbBToCgCYNmanqOjOrHT27BAREZkHw44FGS5hBXgEwMPFQ7rxlWHqeQVhx3BPLBcXoF49a1VJRETk2Bh2LKjcJazUVOnGV2p1hdepyo7X4YLVRERE5sGwY0GVzsRq0ABwLr+eIwcnExERmR/DjgUZ7osV7lO9mVgcnExERGQ+DDsWVGnPDldPJiIishqGHQu6mlu9aedcPZmIiMj8FA07u3btwlNPPYWwsDCoVCqsXbu23DGnTp3C008/DY1GAy8vL3Tq1AmXDaEBwO3bt5GQkICAgAB4e3tj0KBBSDNMa1LYo5GP4pnmzyAqMErawAUFiYiIrE7RsJOfn4+2bdti4cKFFe4/f/48HnnkEbRo0QI7d+7EsWPHMH36dLi7u8vHjBs3Dj/99BNWrVqFpKQkXL9+HQMHDrRWE6o05ZEpWPv8Wjwc/rC0gWGHiIjI6lRCCKF0EQCgUqmwZs0a9O/fX972/PPPw8XFBd98802F79FqtQgMDMS3336Lv/3tbwCA06dPo2XLlti3bx8efvhhkz47JycHGo0GWq0Wvr6+tW5LperXBzIygOPHgdaty+1u3Bi4eBHYtw8wsXQiIqI6y9Tvb5sds6PX67Fx40Y0a9YMsbGxCAoKQufOnY0udR06dAg6nQ4xMTHythYtWiAiIgL79u2r9NxFRUXIyckxelhcfr4UdIAKe3aEYM8OERGRJdhs2ElPT0deXh7mzJmDPn36YOvWrRgwYAAGDhyIpKQkAEBqaipcXV3h5+dn9N7g4GCkGpJDBWbPng2NRiM/GjZsaMmmSAyXsDQaoIL0qdUCt29Lz4ODLV8OERFRXWGzYUev1wMAnnnmGYwbNw7t2rXDlClT0K9fPyxevLhW5546dSq0Wq38uGK4hYMlmTheR6MBPDwsXw4REVFdUX4ZXxtRv359ODs7Iyoqymh7y5YtsXv3bgBASEgIiouLkZ2dbdS7k5aWhpAqrgW5ubnBzc3NInVXioOTiYiIFGGzPTuurq7o1KkTUlJSjLafOXMGkX8tytehQwe4uLhg+/bt8v6UlBRcvnwZ0dHRVq33nrh6MhERkSIU7dnJy8vDuXPn5NcXLlzAkSNH4O/vj4iICEyaNAmDBw9G9+7d0bNnT2zevBk//fQTdu7cCQDQaDQYNmwYxo8fD39/f/j6+mLMmDGIjo42eSaW1XD1ZCIiIkUoGnYOHjyInj17yq/Hjx8PAIiPj8fSpUsxYMAALF68GLNnz8brr7+O5s2b48cff8Qjjzwiv+fTTz+Fk5MTBg0ahKKiIsTGxuKLL76welvuiasnExERKcJm1tlRklXW2WnSBPjzT2D3bqBr13K74+OB//4XmDMHmDzZMiUQERE5ErtfZ8eh6PWAYcYXBygTERFZFcOONaSlATodoFZXOgKZA5SJiIgsg2HHGi5dkn42aAA4VzxMij07RERElsGwYw33GJxcUgLcvCk9Z9ghIiIyL4Yda7hH2ElPl+6NpVYDAQFWrIuIiKgOYNixBhMXFAwKkgIPERERmQ/DjjVwQUEiIiLFMOxYA28VQUREpBiGHWvg6slERESKYdixtPx8ICNDes4FBYmIiKyOYcfSDCsnazRAJUtZM+wQERFZDsOOpd3jEhbAsENERGRJDDuWZlg92YSwwwHKRERE5sewY2ns2SEiIlIUw46l3SPs5OVJD4Bhh4iIyBIYdizNxDV2vLwAb28r1URERFSHMOxYGldPJiIiUhTDjiXp9XemnnP1ZCIiIkUw7FhSWhqg00l396wkzbBnh4iIyLIYdizJcAmrQQPA2bnCQ3irCCIiIsti2LEkTjsnIiJSHMOOJTHsEBERKY5hx5K4ejIREZHiGHYsiT07REREimPYsSRvbyAoqNKwU1oqTdgCGHaIiIgspeIpQmQe//tflbszMqTAo1IBgYFWqomIiKiOYc+OggyXsOrXB1xclK2FiIjIUTHsKIiDk4mIiCyPYUdBHJxMRERkeQw7CuLqyURERJbHsKMg9uwQERFZHsOOghh2iIiILI9hR0EcoExERGR5DDsKYs8OERGR5THsKIgDlImIiCyPYUchhYWAVis9Z9ghIiKyHIYdhRjuieXmBmg0ytZCRETkyBh2FFJ2vI5KpWwtREREjoxhRyGciUVERGQdDDsK4eBkIiIi62DYUQinnRMREVkHw45CGHaIiIisg2FHIQw7RERE1sGwoxAOUCYiIrIOhh2FcIAyERGRdTDsKEAIXsYiIiKyFoYdBWRlATqd9Dw4WNlaiIiIHB3DjgIMvTr16km3iyAiIiLLYdhRAAcnExERWQ/DjgI4OJmIiMh6GHYUwMHJRERE1sOwowCGHSIiIuth2FEAww4REZH1MOwogAOUiYiIrIdhRwEcoExERGQ9DDsK4GUsIiIi62HYsbLiYiAjQ3rOsENERGR5DDtWlp4u/XR2Bvz9la2FiIioLmDYsbKyl7Cc+K9PRERkcc5KF1DXcHAyEdUFpaWl0BnueExUQy4uLlCr1bU+D8OOlXFwMhE5MiEEUlNTkZ2drXQp5CD8/PwQEhIClUpV43Mw7FgZww4ROTJD0AkKCoKnp2etvqCobhNCoKCgAOl/DXYNrcXidAw7VsawQ0SOqrS0VA46AQEBSpdDDsDDwwMAkJ6ejqCgoBpf0uIQWStj2CEiR2UYo+Pp6alwJeRIDL9PtRkDpmjY2bVrF5566imEhYVBpVJh7dq1lR47atQoqFQqfPbZZ0bbMzMzERcXB19fX/j5+WHYsGHIy8uzbOG1YBigzFtFEJGj4qUrMidz/D4pGnby8/PRtm1bLFy4sMrj1qxZg99++w1hYWHl9sXFxeHEiRPYtm0bNmzYgF27dmHkyJGWKrnW2LNDRERkXYqGnSeeeALvvfceBgwYUOkx165dw5gxY7B8+XK4uLgY7Tt16hQ2b96Mf//73+jcuTMeeeQRLFiwACtWrMD169ctXX61CcGwQ0Rkb3r06IGxY8ea5VwXL16ESqXCkSNHzHI+Mo1Nj9nR6/UYMmQIJk2ahFatWpXbv2/fPvj5+aFjx47ytpiYGDg5OWH//v2VnreoqAg5OTlGD2vIzQUKC6XnwcFW+UgiIjLB0KFDoVKpyj3OnTundGlkBjYddv75z3/C2dkZr7/+eoX7U1NTERQUZLTN2dkZ/v7+SDV0oVRg9uzZ0Gg08qNhw4ZmrbsyhpJ8fAAvL6t8JBERmahPnz64ceOG0aNx48ZKl0VmYLNh59ChQ5g3bx6WLl1q9sFuU6dOhVarlR9Xrlwx6/krYwg7HJxMRHWFEAL5xfmKPIQQ1arVzc0NISEhRo+Kpjp/88036NixI3x8fBASEoK///3v8lowAJCVlYW4uDgEBgbCw8MDTZs2xZIlS4zO8eeff6Jnz57w9PRE27ZtsW/fvpr9A5NJbHadnV9//RXp6emIiIiQt5WWlmLChAn47LPPcPHiRYSEhBj9ggFASUkJMjMzEVLFoBg3Nze4ublZrPbK8FYRRFTXFOgK4D3bW5HPzpuaBy9X83ej63Q6zJo1C82bN0d6ejrGjx+PoUOH4ueffwYATJ8+HSdPnsSmTZtQv359nDt3DoWGMQx/mTZtGubOnYumTZti2rRpeOGFF3Du3Dk4O9vs17Jds9l/1SFDhiAmJsZoW2xsLIYMGYKXX34ZABAdHY3s7GwcOnQIHTp0AADs2LEDer0enTt3tnrN98LByUREtmvDhg3w9r4TzJ544gmsWrWq3HGvvPKK/Py+++7D/Pnz0alTJ+Tl5cHb2xuXL19G+/bt5fGkjRo1KneOiRMnom/fvgCAd955B61atcK5c+fQokULM7eKAIXDTl5entHgrwsXLuDIkSPw9/dHREREuRU4XVxcEBISgubNmwMAWrZsiT59+mDEiBFYvHgxdDodRo8ejeeff77CaepKY9ghorrG08UTeVOVWfvM06V6ixv27NkTixYtkl97VTK48tChQ5g5cyaOHj2KrKws6PV6AMDly5cRFRWF1157DYMGDcLhw4fRu3dv9O/fH126dDE6xwMPPCA/N9wGIT09nWHHQhQNOwcPHkTPnj3l1+PHjwcAxMfHY+nSpSadY/ny5Rg9ejR69eoFJycnDBo0CPPnz7dEubXGsENEdY1KpbLIpSRL8PLywv3331/lMfn5+YiNjUVsbCyWL1+OwMBAXL58GbGxsSguLgYg9QhdunQJP//8M7Zt24ZevXohISEBc+fOlc9TdikVw7hUQ2gi81M07PTo0aNaA8guXrxYbpu/vz++/fZbM1ZlORygTERk306fPo2MjAzMmTNHnsl78ODBcscFBgYiPj4e8fHx6NatGyZNmmQUdsi6bHbMjiPiAGUiIvsWEREBV1dXLFiwAKNGjUJycjJmzZpldMyMGTPQoUMHtGrVCkVFRdiwYQNatmypUMUE2PDUc0fEy1hERPYtMDAQS5cuxapVqxAVFYU5c+aU67FxdXXF1KlT8cADD6B79+5Qq9VYsWKFQhUTAKhEdRcicEA5OTnQaDTQarXw9fW1yGeUlgKuroBeL/XwMPAQkaO5ffs2Lly4gMaNG8Pd3V3pcshBVPV7Zer3N3t2rOTmTSnoODkBgYFKV0NERFR3MOxYieESVlAQUMGCnERERGQhDDtWwsHJREREymDYsRIOTiYiIlIGw46VMOwQEREpg2HHShh2iIiIlMGwYyVcPZmIiEgZDDtWwgHKREREymDYsRJexiIiIlIGw46VMOwQETmOmTNnol27dkqXUSMFBQUYNGgQfH19oVKpkJ2drXRJFsewYwX5+UBurvScYYeIyLaoVKoqHzNnziz3nokTJ2L79u3y66FDh6J///7WK7oWli1bhl9//RV79+7FjRs3kJWVBZVKhSNHjihdmsXwrudWkJYm/fTwAHx8lK2FiIiM3TAMqgSwcuVKzJgxAykpKfI2b29v+bkQAqWlpfD29jbabk/Onz+Pli1bonXr1gCAixcvKluQFbBnxwoM/x2FhgIqlbK1EBFZkxBS77YSD1Nvcx0SEiI/NBoNVCqV/Pr06dPw8fHBpk2b0KFDB7i5uWH37t1Gl7FmzpyJZcuWYd26dXJv0M6dOwEAx48fx2OPPQYPDw8EBARg5MiRyMvLkz/b0CM0d+5chIaGIiAgAAkJCdDpdJXWe/ToUfTs2RM+Pj7w9fVFhw4dcPDgQXn/jz/+iFatWsHNzQ2NGjXCxx9/LO/r0aMHPv74Y+zatQsqlQo9evRA48aNAQDt27eXt5Wt7YMPPkBwcDD8/Pzw7rvvoqSkBJMmTYK/vz/Cw8OxZMkSo/omT56MZs2awdPTE/fddx+mT58ut0cIgZiYGMTGxsJwH/LMzEyEh4djxowZpv0PVgPs2bECjtchorqqoABQqgMkLw/w8jLPuaZMmYK5c+fivvvuQ7169eQwA0iXtE6dOoWcnBz5i9/f3x/5+fmIjY1FdHQ0fv/9d6Snp2P48OEYPXo0li5dKr8/MTERoaGhSExMxLlz5zB48GC0a9cOI0aMqLCWuLg4tG/fHosWLYJarcaRI0fg4uICADh06BCee+45zJw5E4MHD8bevXvxf//3fwgICMDQoUOxevVqTJkyBcnJyVi9ejVcXV1x/vx5PPTQQ/jll1/QqlUruLq6yp+1Y8cOhIeHY9euXdizZw+GDRuGvXv3onv37ti/fz9WrlyJV199FY8//jjCw8MBAD4+Pli6dCnCwsJw/PhxjBgxAj4+PvjHP/4BlUqFZcuWoU2bNpg/fz7eeOMNjBo1Cg0aNLBo2IEgodVqBQCh1Wotcv7PPxcCEGLgQIucnojIJhQWFoqTJ0+KwsJCeVtenvT3T4lHXl7127BkyRKh0Wjk14mJiQKAWLt2rdFxb7/9tmjbtq38Oj4+XjzzzDNGx3z11VeiXr16Iq9MIRs3bhROTk4iNTVVfl9kZKQoKSmRj3n22WfF4MGDK63Rx8dHLF26tMJ9f//738Xjjz9utG3SpEkiKipKfv3GG2+IRx99VH594cIFAUD88ccfRu8z1FZaWipva968uejWrZv8uqSkRHh5eYnvvvuu0no/+ugj0aFDB6Nt33//vXB3dxdTpkwRXl5e4syZM5W+v6LfKwNTv7/Zs2MF7NkhorrK01PqYVHqs82lY8eO1X7PqVOn0LZtW3iV6V7q2rUr9Ho9UlJSEBwcDABo1aoV1Gq1fExoaCiOHz9e6XnHjx+P4cOH45tvvkFMTAyeffZZNGnSRP7MZ555xuj4rl274rPPPkNpaanR55iiVatWcHK6M+IlODhYHusDAGq1GgEBAUhPT5e3rVy5EvPnz8f58+eRl5eHkpIS+Pr6Gp332WefxZo1azBnzhwsWrQITZs2rVZd1cUxO1bAsENEdZVKJV1KUuJhzjGSXua6HlYBwyUoA5VKBb1eX+nxM2fOxIkTJ9C3b1/s2LEDUVFRWLNmjdVqq6reffv2IS4uDk8++SQ2bNiAP/74A9OmTUNxcbHRewoKCnDo0CGo1WqcPXvWIrWXxbBjBWUHKBMRkeNxdXVFaWmp0baWLVvi6NGjyM/Pl7ft2bMHTk5OaN68ea0+r1mzZhg3bhy2bt2KgQMHymOFWrZsiT179hgdu2fPHjRr1qzSXh3DGJ2766+JvXv3IjIyEtOmTUPHjh3RtGlTXLp0qdxxEyZMgJOTEzZt2oT58+djx44dtf7sqjDsWAF7doiIHFujRo1w7NgxpKSk4NatW9DpdIiLi4O7uzvi4+ORnJyMxMREjBkzBkOGDJEvYVVXYWEhRo8ejZ07d+LSpUvYs2cPfv/9d7Rs2RKAFCK2b9+OWbNm4cyZM1i2bBk+//xzTJw4sdJzBgUFwcPDA5s3b0ZaWhq0Wm2NagOApk2b4vLly1ixYgXOnz+P+fPnl+t12rhxI77++mssX74cjz/+OCZNmoT4+HhkZWXV+HPvhWHHChh2iIgc24gRI9C8eXN07NgRgYGB2LNnDzw9PbFlyxZkZmaiU6dO+Nvf/oZevXrh888/r/HnqNVqZGRk4KWXXkKzZs3w3HPP4YknnsA777wDAHjwwQfx/fffY8WKFWjdujVmzJiBd999F0OHDq30nM7Ozpg/fz6+/PJLhIWFlRvzUx1PP/00xo0bh9GjR6Ndu3bYu3cvpk+fLu+/efMmhg0bhpkzZ+LBBx8EALzzzjsIDg7GqFGjavy596ISwtSVCBxXTk4ONBoNtFptuUFUtaXXA25uQEkJcOUK8NfMPCIih3P79m1cuHABjRs3hru7u9LlkIOo6vfK1O9v9uxYWGamFHQAIChI2VqIiIjqIoYdCzMMTq5fHyizThMRERFZCcOOhXG8DhERkbIYdiyMYYeIiEhZDDsWxrBDRESkLIYdC2PYISIiUhbDjoVx9WQiIiJlMexYGHt2iIiIlMWwY2EMO0RERMpi2LEwhh0iIsczc+ZMtGvXTukyylm6dCn8/PyULgMAMHToUPTv37/KY3r06IGxY8davBaGHQsqKgIM9zVj2CEisk0qlarKx8yZM8u9Z+LEidi+fbv82pQvdmsYPHgwzpw5o3QZAIB58+Zh6dKlSpcBAHBWugBHZujVcXUF6tVTthYiIqrYDcNMEgArV67EjBkzkJKSIm/z9vaWnwshUFpaCm9vb6PttsLDwwMeHh5KlwEA0Gg0SpcgY8+OBZW9hKVSKVsLEZEihADy85V5mHif65CQEPmh0WigUqnk16dPn4aPjw82bdqEDh06wM3NDbt37za6jDVz5kwsW7YM69atk3uDdu7cCQA4fvw4HnvsMXh4eCAgIAAjR45EXl6e/NmGHqG5c+ciNDQUAQEBSEhIgE6nq7Teo0ePomfPnvDx8YGvry86dOiAgwcPAqj4MtZ7772HoKAg+Pj4YPjw4ZgyZYrRJThDDR988AGCg4Ph5+eHd999FyUlJZg0aRL8/f0RHh6OJUuWGJ3X1LYZ5Ofn46WXXoK3tzdCQ0Px8ccfm/S/jzmwZ8eCOF6HiOq8ggJAqR6QvDzAy8ssp5oyZQrmzp2L++67D/Xq1ZPDDCBd0jp16hRycnLkQODv74/8/HzExsYiOjoav//+O9LT0zF8+HCMHj3a6PJOYmIiQkNDkZiYiHPnzmHw4MFo164dRowYUWEtcXFxaN++PRYtWgS1Wo0jR47AxcWlwmOXL1+O999/H1988QW6du2KFStW4OOPP0bjxo2NjtuxYwfCw8Oxa9cu7NmzB8OGDcPevXvRvXt37N+/HytXrsSrr76Kxx9/HOHh4Sa3raxJkyYhKSkJ69atQ1BQEN58800cPnzYOmOfBAmtVisACK1Wa9bzLl4sBCDE00+b9bRERDapsLBQnDx5UhQWFt7ZmJcn/SFU4pGXV+02LFmyRGg0Gvl1YmKiACDWrl1rdNzbb78t2rZtK7+Oj48XzzzzjNExX331lahXr57IK1PHxo0bhZOTk0hNTZXfFxkZKUpKSuRjnn32WTF48OBKa/Tx8RFLly41qf7OnTuLhIQEo2O6du1arvbIyEhRWloqb2vevLno1q2b/LqkpER4eXmJ7777rlptM/yb5ObmCldXV/H999/Lx2dkZAgPDw/xxhtvVNpWISr5vfqLqd/fvIxlQezZIaI6z9NT6mFR4uHpabZmdOzYsdrvOXXqFNq2bQuvMr1LXbt2hV6vNxoT1KpVK6jVavl1aGgo0tPTKz3v+PHjMXz4cMTExGDOnDk4f/58pcempKTgoYceMtp292tDDU5OdyJBcHAw2rRpI79Wq9UICAiQ6zK1bQbnz59HcXExOnfuLG/z9/dH8+bNK63dnBh2LIirJxNRnadSSZeSlHiYcbBk2S91c7v7EpRKpYJer6/0+JkzZ+LEiRPo27cvduzYgaioKKxZs8bsNVS3LlvGsGNB7NkhIqobXF1dUVpaarStZcuWOHr0KPLz8+Vte/bsgZOTU617NJo1a4Zx48Zh69atGDhwYLnBwwbNmzfH77//brTt7tc1Ud22NWnSBC4uLti/f7+8LSsry2rT5Bl2LKi0FHB2ZtghInJ0jRo1wrFjx5CSkoJbt25Bp9MhLi4O7u7uiI+PR3JyMhITEzFmzBgMGTIEwcHBNfqcwsJCjB49Gjt37sSlS5ewZ88e/P7772jZsmWFx48ZMwb/+c9/sGzZMpw9exbvvfcejh07BlUte72q2zZvb28MGzYMkyZNwo4dO5CcnIyhQ4caXTqzJM7GsqCffgL0eulBRESOa8SIEdi5cyc6duyIvLw8JCYmokePHtiyZQveeOMNdOrUCZ6enhg0aBA++eSTGn+OWq1GRkYGXnrpJaSlpaF+/foYOHAg3nnnnQqPj4uLw59//omJEyfi9u3beO655zB06FAcOHCgxjUAgKenZ7Xb9tFHHyEvLw9PPfUUfHx8MGHCBGi12lrVYSqVECYuRODAcnJyoNFooNVq4evrq3Q5RER26fbt27hw4QIaN24Md3d3pcuhSjz++OMICQnBN998o3QpJqnq98rU72/27BARETmogoICLF68GLGxsVCr1fjuu+/wyy+/YNu2bUqXZlUMO0RERA5KpVLh559/xvvvv4/bt2+jefPm+PHHHxETE6N0aVbFsENEROSgPDw88MsvvyhdhuI4G4uIiIgcGsMOERGZFee9kDmZ4/eJYYeIiMzCsOJuQUGBwpWQIzH8PlV2s1NTcMwOERGZhVqthp+fn3z/JE9Pz1ovXkd1lxACBQUFSE9Ph5+fn9H9w6qLYYeIiMwm5K8l46u6kSVRdfj5+cm/VzXFsENERGajUqkQGhqKoKAg6HQ6pcshO+fi4lKrHh0Dhh0iIjI7tVptli8pInPgAGUiIiJyaAw7RERE5NAYdoiIiMihccwO7ixYlJOTo3AlREREZCrD9/a9Fh5k2AGQm5sLAGjYsKHClRAREVF15ebmQqPRVLpfJbiuN/R6Pa5fvw4fH58KF8DKyclBw4YNceXKFfj6+ipQoeU5ehsdvX0A2+go2EbH4OhttJX2CSGQm5uLsLAwODlVPjKHPTsAnJycEB4efs/jfH19HfKXtixHb6Ojtw9gGx0F2+gYHL2NttC+qnp0DDhAmYiIiBwaww4RERE5NIYdE7i5ueHtt9+Gm5ub0qVYjKO30dHbB7CNjoJtdAyO3kZ7ax8HKBMREZFDY88OEREROTSGHSIiInJoDDtERETk0Bh2iIiIyKEx7NzDwoUL0ahRI7i7u6Nz5844cOCA0iVVaPbs2ejUqRN8fHwQFBSE/v37IyUlxeiY27dvIyEhAQEBAfD29sagQYOQlpZmdMzly5fRt29feHp6IigoCJMmTUJJSYnRMTt37sSDDz4INzc33H///Vi6dKmlm1ehOXPmQKVSYezYsfI2R2jjtWvX8OKLLyIgIAAeHh5o06YNDh48KO8XQmDGjBkIDQ2Fh4cHYmJicPbsWaNzZGZmIi4uDr6+vvDz88OwYcOQl5dndMyxY8fQrVs3uLu7o2HDhvjwww8t3rbS0lJMnz4djRs3hoeHB5o0aYJZs2YZ3dfG3tq3a9cuPPXUUwgLC4NKpcLatWuN9luzPatWrUKLFi3g7u6ONm3a4Oeff7Z4G3U6HSZPnow2bdrAy8sLYWFheOmll3D9+nWHaePdRo0aBZVKhc8++8xouyO08dSpU3j66aeh0Wjg5eWFTp064fLly/J+u/0bK6hSK1asEK6uruLrr78WJ06cECNGjBB+fn4iLS1N6dLKiY2NFUuWLBHJycniyJEj4sknnxQREREiLy9PPmbUqFGiYcOGYvv27eLgwYPi4YcfFl26dJH3l5SUiNatW4uYmBjxxx9/iJ9//lnUr19fTJ06VT7mzz//FJ6enmL8+PHi5MmTYsGCBUKtVovNmzdbtb0HDhwQjRo1Eg888IB444035O323sbMzEwRGRkphg4dKvbv3y/+/PNPsWXLFnHu3Dn5mDlz5giNRiPWrl0rjh49Kp5++mnRuHFjUVhYKB/Tp08f0bZtW/Hbb7+JX3/9Vdx///3ihRdekPdrtVoRHBws4uLiRHJysvjuu++Eh4eH+PLLLy3avvfff18EBASIDRs2iAsXLohVq1YJb29vMW/ePLtt388//yymTZsmVq9eLQCINWvWGO23Vnv27Nkj1Gq1+PDDD8XJkyfFW2+9JVxcXMTx48ct2sbs7GwRExMjVq5cKU6fPi327dsnHnroIdGhQwejc9hzG8tavXq1aNu2rQgLCxOffvqpQ7Xx3Llzwt/fX0yaNEkcPnxYnDt3Tqxbt87oO89e/8Yy7FThoYceEgkJCfLr0tJSERYWJmbPnq1gVaZJT08XAERSUpIQQvqD5OLiIlatWiUfc+rUKQFA7Nu3Twgh/Yfg5OQkUlNT5WMWLVokfH19RVFRkRBCiH/84x+iVatWRp81ePBgERsba+kmyXJzc0XTpk3Ftm3bxKOPPiqHHUdo4+TJk8UjjzxS6X69Xi9CQkLERx99JG/Lzs4Wbm5u4rvvvhNCCHHy5EkBQPz+++/yMZs2bRIqlUpcu3ZNCCHEF198IerVqye32fDZzZs3N3eTjPTt21e88sorRtsGDhwo4uLihBD23767v0Cs2Z7nnntO9O3b16iezp07i1dffdWibazIgQMHBABx6dIlIYTjtPHq1auiQYMGIjk5WURGRhqFHUdo4+DBg8WLL75Y6Xvs+W8sL2NVori4GIcOHUJMTIy8zcnJCTExMdi3b5+ClZlGq9UCAPz9/QEAhw4dgk6nM2pPixYtEBERIbdn3759aNOmDYKDg+VjYmNjkZOTgxMnTsjHlD2H4Rhr/pskJCSgb9++5epwhDauX78eHTt2xLPPPougoCC0b98e//rXv+T9Fy5cQGpqqlF9Go0GnTt3Nmqjn58fOnbsKB8TExMDJycn7N+/Xz6me/fucHV1lY+JjY1FSkoKsrKyLNa+Ll26YPv27Thz5gwA4OjRo9i9ezeeeOIJh2jf3azZHlv4b9NAq9VCpVLBz89Prs3e26jX6zFkyBBMmjQJrVq1Krff3tuo1+uxceNGNGvWDLGxsQgKCkLnzp2NLnXZ899Yhp1K3Lp1C6WlpUb/gwFAcHAwUlNTFarKNHq9HmPHjkXXrl3RunVrAEBqaipcXV3lPz4GZduTmppaYXsN+6o6JicnB4WFhZZojpEVK1bg8OHDmD17drl9jtDGP//8E4sWLULTpk2xZcsWvPbaa3j99dexbNkyoxqr+r1MTU1FUFCQ0X5nZ2f4+/tX69/BEqZMmYLnn38eLVq0gIuLC9q3b4+xY8ciLi7O6LPttX13s2Z7KjvG2n+vbt++jcmTJ+OFF16QbxDpCG385z//CWdnZ7z++usV7rf3NqanpyMvLw9z5sxBnz59sHXrVgwYMAADBw5EUlKSXJu9/o3lXc8dUEJCApKTk7F7926lSzGrK1eu4I033sC2bdvg7u6udDkWodfr0bFjR3zwwQcAgPbt2yM5ORmLFy9GfHy8wtXV3vfff4/ly5fj22+/RatWrXDkyBGMHTsWYWFhDtG+uk6n0+G5556DEAKLFi1SuhyzOXToEObNm4fDhw9DpVIpXY5F6PV6AMAzzzyDcePGAQDatWuHvXv3YvHixXj00UeVLK/W2LNTifr160OtVpcbZZ6WloaQkBCFqrq30aNHY8OGDUhMTER4eLi8PSQkBMXFxcjOzjY6vmx7QkJCKmyvYV9Vx/j6+sLDw8PczTFy6NAhpKen48EHH4SzszOcnZ2RlJSE+fPnw9nZGcHBwXbfxtDQUERFRRlta9mypTwbwlBjVb+XISEhSE9PN9pfUlKCzMzMav07WMKkSZPk3p02bdpgyJAhGDdunNxTZ+/tu5s121PZMdZqryHoXLp0Cdu2bZN7dQy12XMbf/31V6SnpyMiIkL+23Pp0iVMmDABjRo1kmuz5zbWr18fzs7O9/z7Y69/Yxl2KuHq6ooOHTpg+/bt8ja9Xo/t27cjOjpawcoqJoTA6NGjsWbNGuzYsQONGzc22t+hQwe4uLgYtSclJQWXL1+W2xMdHY3jx48b/Qdr+KNl+A8gOjra6ByGY6zxb9KrVy8cP34cR44ckR8dO3ZEXFyc/Nze29i1a9dySwacOXMGkZGRAIDGjRsjJCTEqL6cnBzs37/fqI3Z2dk4dOiQfMyOHTug1+vRuXNn+Zhdu3ZBp9PJx2zbtg3NmzdHvXr1LNa+goICODkZ/9lRq9Xy/6u09/bdzZrtUfL31hB0zp49i19++QUBAQFG++29jUOGDMGxY8eM/vaEhYVh0qRJ2LJli0O00dXVFZ06dary749df49YbOizA1ixYoVwc3MTS5cuFSdPnhQjR44Ufn5+RqPMbcVrr70mNBqN2Llzp7hx44b8KCgokI8ZNWqUiIiIEDt27BAHDx4U0dHRIjo6Wt5vmDLYu3dvceTIEbF582YRGBhY4ZTBSZMmiVOnTomFCxcqMvXcoOxsLCHsv40HDhwQzs7O4v333xdnz54Vy5cvF56enuJ///uffMycOXOEn5+fWLdunTh27Jh45plnKpzK3L59e7F//36xe/du0bRpU6MpsNnZ2SI4OFgMGTJEJCcnixUrVghPT0+LTz2Pj48XDRo0kKeer169WtSvX1/84x//sNv25ebmij/++EP88ccfAoD45JNPxB9//CHPRLJWe/bs2SOcnZ3F3LlzxalTp8Tbb79ttinLVbWxuLhYPP300yI8PFwcOXLE6O9P2VlH9tzGitw9G8sR2rh69Wrh4uIivvrqK3H27Fl5Svivv/4qn8Ne/8Yy7NzDggULREREhHB1dRUPPfSQ+O2335QuqUIAKnwsWbJEPqawsFD83//9n6hXr57w9PQUAwYMEDdu3DA6z8WLF8UTTzwhPDw8RP369cWECROETqczOiYxMVG0a9dOuLq6ivvuu8/oM6zt7rDjCG386aefROvWrYWbm5to0aKF+Oqrr4z26/V6MX36dBEcHCzc3NxEr169REpKitExGRkZ4oUXXhDe3t7C19dXvPzyyyI3N9fomKNHj4pHHnlEuLm5iQYNGog5c+ZYvG05OTnijTfeEBEREcLd3V3cd999Ytq0aUZfivbWvsTExAr/24uPj7d6e77//nvRrFkz4erqKlq1aiU2btxo8TZeuHCh0r8/iYmJDtHGilQUdhyhjf/5z3/E/fffL9zd3UXbtm3F2rVrjc5hr39jVUKUWbqUiIiIyMFwzA4RERE5NIYdIiIicmgMO0REROTQGHaIiIjIoTHsEBERkUNj2CEiIiKHxrBDREREDo1hh4iIiBwaww4ROYyhQ4eif//+SpdBRDaGKygTkcPQarUQQsDPz0/pUojIhjDsEBERkUPjZSwisjs//PAD2rRpAw8PDwQEBCAmJgb5+flGl7EuXrwIlUpV7tGjRw/5PLt370a3bt3g4eGBhg0b4vXXX0d+fr4yjSIii2HYISK7cuPGDbzwwgt45ZVXcOrUKezcuRMDBw7E3Z3UDRs2xI0bN+THH3/8gYCAAHTv3h0AcP78efTp0weDBg3CsWPHsHLlSuzevRujR49WollEZEG8jEVEduXw4cPo0KEDLl68iMjISKN9Q4cORXZ2NtauXWu0/fbt2+jRowcCAwOxbt06ODk5Yfjw4VCr1fjyyy/l43bv3o1HH30U+fn5cHd3t0ZziMgKnJUugIioOtq2bYtevXqhTZs2iI2NRe/evfG3v/0N9erVq/Q9r7zyCnJzc7Ft2zY4OUkd2kePHsWxY8ewfPly+TghBPR6PS5cuICWLVtavC1EZB0MO0RkV9RqNbZt24a9e/di69atWLBgAaZNm4b9+/dXePx7772HLVu24MCBA/Dx8ZG35+Xl4dVXX8Xrr79e7j0REREWq5+IrI+XsYjIrpWWliIyMhLjx4/HsWPHjC5j/fjjj3jhhRewadMm9OrVy+h9cXFxSEtLwy+//KJA1URkTRygTER2Zf/+/fjggw9w8OBBXL58GatXr8bNmzfLXXZKTk7GSy+9hMmTJ6NVq1ZITU1FamoqMjMzAQCTJ0/G3r17MXr0aBw5cgRnz57FunXrOECZyAEx7BCRXfH19cWuXbvw5JNPolmzZnjrrbfw8ccf44knnjA67uDBgygoKMB7772H0NBQ+TFw4EAAwAMPPICkpCScOXMG3bp1Q/v27TFjxgyEhYUp0SwisiBexiIiIiKHxp4dIiIicmgMO0REROTQGHaIiIjIoTHsEBERkUNj2CEiIiKHxrBDREREDo1hh4iIiBwaww4RERE5NIYdIiIicmgMO0REROTQGHaIiIjIof0/wjmJxyORkuAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash-attn-performance:\n",
      "      size       Flash  Triton softmax  Triton sigmoid\n",
      "0    512.0  161.207526      126.884806      151.784445\n",
      "1   1024.0  197.187537      168.056774      184.875439\n",
      "2   2048.0  210.833889      177.795375      202.092612\n",
      "3   4096.0  213.190950      187.391424      206.451322\n",
      "4   8192.0  215.748900      190.762378      208.767185\n",
      "5  16384.0  216.769739      190.766615      209.917637\n"
     ]
    }
   ],
   "source": [
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92f33f-ead3-4753-a9c9-26c817e8d723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
